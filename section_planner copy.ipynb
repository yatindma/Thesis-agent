{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea9ace7",
   "metadata": {},
   "source": [
    "# Hierarchical Section Planning Notebook\n",
    "\n",
    "This notebook contains code to:\n",
    "1. Parse a raw index string into a hierarchical JSON-like structure.\n",
    "2. Chunk top-level sections into groups (e.g., five at a time).\n",
    "3. Build prompts for an LLM to generate a plan for each section.\n",
    "4. Call the LLM to fill in plans and return a complete JSON structure.\n",
    "\n",
    "Replace the placeholder `llm_client` and `AZURE_OPENAI_MODEL_NAME` with your actual client and model details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6cb7634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "import json\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "import json\n",
    "from openai import AzureOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd84851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e968404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured LLM Response:\n",
      "sections=[SectionNode(title='Introduction', section='1', level=0, children=[SectionNode(title='Motivation and Objectives', section='1.1', level=1, children=[]), SectionNode(title='Thesis Scope and Contributions', section='1.2', level=1, children=[]), SectionNode(title='Thesis Organization', section='1.3', level=1, children=[])]), SectionNode(title='Background and Foundations', section='2', level=0, children=[SectionNode(title='Domain Background: Conversational Agents in Higher Education', section='2.1', level=1, children=[SectionNode(title='Evolution and Definitions of Conversational Agents', section='2.1.1', level=2, children=[]), SectionNode(title='Use Cases and Impact in University Administration', section='2.1.2', level=2, children=[])]), SectionNode(title='Technical Foundations: LLM Architectures and Methods', section='2.2', level=1, children=[SectionNode(title='Transformer Architecture Overview', section='2.2.1', level=2, children=[]), SectionNode(title='Pretraining Paradigms and Domain Adaptation', section='2.2.2', level=2, children=[]), SectionNode(title='Parameter-Efficient Fine-Tuning (LoRA)', section='2.2.3', level=2, children=[SectionNode(title='LoRA Methodology and Mechanisms', section='2.2.3.1', level=3, children=[]), SectionNode(title='Comparison with Other PEFT Techniques', section='2.2.3.2', level=3, children=[])]), SectionNode(title='Supervised Fine-Tuning vs. Direct Preference Optimization', section='2.2.4', level=2, children=[SectionNode(title='Supervised Fine-Tuning (SFT) Techniques', section='2.2.4.1', level=3, children=[]), SectionNode(title='Direct Preference Optimization (DPO) Principles', section='2.2.4.2', level=3, children=[]), SectionNode(title='Hybrid SFT + DPO Approaches', section='2.2.4.3', level=3, children=[])]), SectionNode(title='Topic Modeling and Content Filtering Techniques', section='2.2.5', level=2, children=[SectionNode(title='Unsupervised Topic Modeling', section='2.2.5.1', level=3, children=[]), SectionNode(title='LLM-Based Theme Classification', section='2.2.5.2', level=3, children=[])])])]), SectionNode(title='Related Work', section='3', level=0, children=[SectionNode(title='Domain-Specific LLMs for Administrative Support', section='3.1', level=1, children=[]), SectionNode(title='Prompt Engineering for User-Aware Generation', section='3.2', level=1, children=[]), SectionNode(title='Evaluation Metrics for Conversational Agents', section='3.3', level=1, children=[]), SectionNode(title='Research Gaps in University Management Chatbots', section='3.4', level=1, children=[])]), SectionNode(title='Data Acquisition and Preprocessing', section='4', level=0, children=[SectionNode(title='FAU Administrative Data Harvesting', section='4.1', level=1, children=[]), SectionNode(title='Text Segmentation into 4 000-Token Chunks', section='4.2', level=1, children=[]), SectionNode(title='Initial QA-Pair Generation with Mistral', section='4.3', level=1, children=[]), SectionNode(title='Two-Stage Filtering Pipeline', section='4.4', level=1, children=[SectionNode(title='Unsupervised Topic Modeling (BERTopic)', section='4.4.1', level=2, children=[]), SectionNode(title='LLM-Based Theme Classification and Lecture Removal', section='4.4.2', level=2, children=[])]), SectionNode(title='Construction of Chosen vs. Rejected Response Sets', section='4.5', level=1, children=[])]), SectionNode(title='Model Fine-Tuning and Prompt Engineering', section='5', level=0, children=[SectionNode(title='Model Selection: LLaMA 3 and Falcon 7B', section='5.1', level=1, children=[SectionNode(title='RAG vs. Role-Aware Fine-Tuning: A Design Justification', section='5.1.1', level=2, children=[])]), SectionNode(title='Supervised Fine-Tuning on Management QA Corpus', section='5.2', level=1, children=[]), SectionNode(title='Direct Preference Optimization (DPO) with QLoRA', section='5.3', level=1, children=[]), SectionNode(title='Custom Prompt Templates and Persona Embedding', section='5.4', level=1, children=[SectionNode(title='Static Persona Injection (User Background & Expert Role)', section='5.4.1', level=2, children=[]), SectionNode(title='Role-Based Persona Modeling (Context-Adaptive Generation)', section='5.4.2', level=2, children=[]), SectionNode(title='Dynamic Context Windows and Memory Traces', section='5.4.3', level=2, children=[])]), SectionNode(title='Implementation Details and Training Infrastructure', section='5.5', level=1, children=[])]), SectionNode(title='Experimental Design and Evaluation', section='6', level=0, children=[SectionNode(title='Evaluation Metrics: Precision, Recall, F1-Score', section='6.1', level=1, children=[]), SectionNode(title='Baseline vs. Fine-Tuned Model Comparisons', section='6.2', level=1, children=[]), SectionNode(title='User-Role Simulation and Contextual Tests', section='6.3', level=1, children=[]), SectionNode(title='Ablation Study on Prompt Components', section='6.4', level=1, children=[]), SectionNode(title='Statistical Significance and Error Analysis', section='6.5', level=1, children=[])]), SectionNode(title='Results and Discussion', section='7', level=0, children=[SectionNode(title='Quantitative Performance Gains (+10 % Precision, +12 % F1)', section='7.1', level=1, children=[]), SectionNode(title='Qualitative Case Studies and Exemplars', section='7.2', level=1, children=[]), SectionNode(title='Limitations of the Current Approach', section='7.3', level=1, children=[]), SectionNode(title='Implications for University Administrative Workflows', section='7.4', level=1, children=[])]), SectionNode(title='Conclusion and Future Work', section='8', level=0, children=[SectionNode(title='Summary of Contributions', section='8.1', level=1, children=[]), SectionNode(title='Recommendations for Deployment', section='8.2', level=1, children=[]), SectionNode(title='Directions for Further Research', section='8.3', level=1, children=[])]), SectionNode(title='List of Abbreviations', section='', level=0, children=[]), SectionNode(title='List of Figures', section='', level=0, children=[]), SectionNode(title='List of Tables', section='', level=0, children=[]), SectionNode(title='References', section='', level=0, children=[])]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 1. Define the Pydantic model for one section node\n",
    "# ----------------------------\n",
    "class SectionNode(BaseModel):\n",
    "    title: str\n",
    "    section: str\n",
    "    level: int\n",
    "    children: List[\"SectionNode\"] = []\n",
    "\n",
    "# Define a response model that wraps the list of sections\n",
    "class SectionHierarchy(BaseModel):\n",
    "    sections: List[SectionNode]\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Function to call the LLM and get a hierarchical JSON from a raw index string\n",
    "# ----------------------------\n",
    "def build_hierarchy_from_index(\n",
    "    raw_index: str, llm_client, model_name: str\n",
    ") -> List[SectionNode]:\n",
    "    \"\"\"\n",
    "    Given a raw index string (e.g., \"1. Introduction\\n1.1 Background\\n1.2 Problem Statement\\n2. Methodology\\n...\"),\n",
    "    call the LLM to parse it into a hierarchical JSON structure matching SectionNode.\n",
    "\n",
    "    Returns a list of SectionNode objects (top‐level sections with nested children).\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Parse the following raw index into a hierarchical structure.  \n",
    "                Each node should have:\n",
    "                - title: the section title\n",
    "                - section: the section number (e.g., \"1.1\", \"2.3.4\") if it's already there use as it's if not then create it. based on the context.\n",
    "                - level: integer where 0 is top-level, 1 is subsection, etc.\n",
    "                - children: array of nested sections with the same structure\n",
    "\n",
    "                For example:\n",
    "                \"1. Introduction\" -> level 0\n",
    "                \"1.1 Background\" -> level 1 (child of Introduction)\n",
    "                \"1.1.1 Historical Context\" -> level 2 (child of Background)\n",
    "                \"1.2 Problem Statement\" -> level 1 (child of Introduction)\n",
    "                \"2. Literature Review\" -> level 0\n",
    "\n",
    "                Raw index:\n",
    "                {raw_index.strip()}\n",
    "                \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm_client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=SectionHierarchy,\n",
    "            temperature=0.0,\n",
    "            max_tokens=4096\n",
    "        )\n",
    "        \n",
    "        # Extract the parsed data\n",
    "        parsed_data = response.choices[0].message.parsed\n",
    "        \n",
    "        # Debug: Print the structured response\n",
    "        print(\"Structured LLM Response:\")\n",
    "        print(parsed_data)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Return the list of sections\n",
    "        return parsed_data.sections\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calling LLM or processing response: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Usage example\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example raw index string (passed separately from context)\n",
    "    raw_index_text = \"\"\"\n",
    "    \n",
    "1  Introduction ........................................................................................................ 1  \n",
    "  1.1 Motivation and Objectives ........................................................................... 2  \n",
    "  1.2 Thesis Scope and Contributions ................................................................. 4  \n",
    "  1.3 Thesis Organization ...................................................................................... 6  \n",
    "\n",
    "2  Background and Foundations ................................................................. 9  \n",
    "  2.1 Domain Background: Conversational Agents in Higher Education ............ 9  \n",
    "    2.1.1 Evolution and Definitions of Conversational Agents .............................. 10  \n",
    "    2.1.2 Use Cases and Impact in University Administration .............................. 11  \n",
    "  2.2 Technical Foundations: LLM Architectures and Methods .................... 12  \n",
    "    2.2.1 Transformer Architecture Overview ................................................ 13  \n",
    "    2.2.2 Pretraining Paradigms and Domain Adaptation ................................. 14  \n",
    "    2.2.3 Parameter-Efficient Fine-Tuning (LoRA) ........................................ 16  \n",
    "      2.2.3.1 LoRA Methodology and Mechanisms ................................................ 17  \n",
    "      2.2.3.2 Comparison with Other PEFT Techniques ....................................... 18  \n",
    "    2.2.4 Supervised Fine-Tuning vs. Direct Preference Optimization............ 19  \n",
    "      2.2.4.1 Supervised Fine-Tuning (SFT) Techniques ......................................... 19  \n",
    "      2.2.4.2 Direct Preference Optimization (DPO) Principles ............................. 20  \n",
    "      2.2.4.3 Hybrid SFT + DPO Approaches .......................................................... 20  \n",
    "    2.2.5 Topic Modeling and Content Filtering Techniques ................................ 21  \n",
    "      2.2.5.1 Unsupervised Topic Modeling ............................................ 22  \n",
    "      2.2.5.2 LLM-Based Theme Classification ...................................................... 23  \n",
    "\n",
    "3  Related Work .................................................................................................... 25  \n",
    "  3.1 Domain-Specific LLMs for Administrative Support .................................. 25  \n",
    "  3.2 Prompt Engineering for User-Aware Generation ....................................... 29  \n",
    "  3.3 Evaluation Metrics for Conversational Agents ............................................ 32  \n",
    "  3.4 Research Gaps in University Management Chatbots ................................. 35  \n",
    "\n",
    "4  Data Acquisition and Preprocessing ........................................................ 39  \n",
    "  4.1 FAU Administrative Data Harvesting ......................................................... 39  \n",
    "  4.2 Text Segmentation into 4 000-Token Chunks ................................................ 42  \n",
    "  4.3 Initial QA-Pair Generation with Mistral .................................................... 45  \n",
    "  4.4 Two-Stage Filtering Pipeline  \n",
    "    4.4.1 Unsupervised Topic Modeling (BERTopic) ......................................... 48  \n",
    "    4.4.2 LLM-Based Theme Classification and Lecture Removal ......................... 51  \n",
    "  4.5 Construction of Chosen vs. Rejected Response Sets .................................. 54  \n",
    "\n",
    "5  Model Fine-Tuning and Prompt Engineering ............................................. 57  \n",
    "  5.1 Model Selection: LLaMA 3 and Falcon 7B .................................................... 57  \n",
    "    5.1.1 RAG vs. Role-Aware Fine-Tuning: A Design Justification  \n",
    "  5.2 Supervised Fine-Tuning on Management QA Corpus .................................. 60  \n",
    "  5.3 Direct Preference Optimization (DPO) with QLoRA .................................... 63  \n",
    "  5.4 Custom Prompt Templates and Persona Embedding .................................... 67  \n",
    "    5.4.1 Static Persona Injection (User Background & Expert Role)  \n",
    "    5.4.2 Role-Based Persona Modeling (Context-Adaptive Generation)  \n",
    "    5.4.3 Dynamic Context Windows and Memory Traces  \n",
    "  5.5 Implementation Details and Training Infrastructure ................................ 70  \n",
    "6  Experimental Design and Evaluation ...................................................... 73  \n",
    "  6.1 Evaluation Metrics: Precision, Recall, F1-Score ............................................ 73  \n",
    "  6.2 Baseline vs. Fine-Tuned Model Comparisons ............................................ 76  \n",
    "  6.3 User-Role Simulation and Contextual Tests .............................................. 80  \n",
    "  6.4 Ablation Study on Prompt Components ..................................................... 83  \n",
    "  6.5 Statistical Significance and Error Analysis .............................................. 86  \n",
    "\n",
    "7  Results and Discussion .................................................................................. 89  \n",
    "  7.1 Quantitative Performance Gains (+10 % Precision, +12 % F1) .................... 89  \n",
    "  7.2 Qualitative Case Studies and Exemplars .................................................... 93  \n",
    "  7.3 Limitations of the Current Approach ......................................................... 97  \n",
    "  7.4 Implications for University Administrative Workflows ........................... 100  \n",
    "\n",
    "8  Conclusion and Future Work ...................................................................... 103  \n",
    "  8.1 Summary of Contributions ............................................................................. 103  \n",
    "  8.2 Recommendations for Deployment .............................................................. 105  \n",
    "  8.3 Directions for Further Research ................................................................. 107  \n",
    "\n",
    "List of Abbreviations ........................................................................................ 110  \n",
    "List of Figures ...................................................................................................... 112  \n",
    "List of Tables ....................................................................................................... 114  \n",
    "References ............................................................................................................ 115  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize your LLM client (replace with your actual Azure OpenAI client)\n",
    "    llm_client = AzureOpenAI(\n",
    "    )\n",
    "\n",
    "    # Call the function to get a hierarchical structure\n",
    "    hierarchy = build_hierarchy_from_index(raw_index_text, llm_client, model_name)\n",
    "\n",
    "    # Print out the result as JSON\n",
    "    # print(json.dumps([node.model_dump() for node in hierarchy], indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Save the result to a file\n",
    "    with open(\"section_hierarchy.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([node.model_dump() for node in hierarchy], f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caae45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we read the content from the above json file\n",
    "# Read the complete content from the pdf file\n",
    "# and pass the compelte text to the LLM with the first 5 sections and ask for the plan what shall come inside it \n",
    "\n",
    "# Read the section hierarchy from the JSON file\n",
    "def read_section_hierarchy(file_path: str) -> List[SectionNode]:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return [SectionNode(**section) for section in data]\n",
    "\n",
    "# read the pdf  \n",
    "def read_pdf_content(file_path: str) -> str:\n",
    "    from PyPDF2 import PdfReader\n",
    "    reader = PdfReader(file_path)\n",
    "    content = []\n",
    "    for page in reader.pages:\n",
    "        content.append(page.extract_text())\n",
    "    return \"\\n\".join(content)\n",
    "\n",
    "# Updated models for better section and subsection planning\n",
    "class SectionPlan(BaseModel):\n",
    "    section_number: str\n",
    "    title: str\n",
    "    level: int\n",
    "    plan: str\n",
    "\n",
    "class BatchSectionPlans(BaseModel):\n",
    "    plans: List[SectionPlan]\n",
    "\n",
    "# Function to flatten all sections and subsections into a single list\n",
    "def flatten_sections(sections: List[SectionNode]) -> List[SectionNode]:\n",
    "    \"\"\"\n",
    "    Flatten the hierarchical structure into a single list containing all sections and subsections.\n",
    "    \"\"\"\n",
    "    flattened = []\n",
    "    \n",
    "    def flatten_recursive(section_list: List[SectionNode]):\n",
    "        for section in section_list:\n",
    "            flattened.append(section)\n",
    "            if section.children:\n",
    "                flatten_recursive(section.children)\n",
    "    \n",
    "    flatten_recursive(sections)\n",
    "    return flattened\n",
    "\n",
    "# Function to group sections into batches of 3 main sections with all their subsections\n",
    "def group_sections_for_batch_processing(sections: List[SectionNode], batch_size: int = 3) -> List[List[SectionNode]]:\n",
    "    \"\"\"\n",
    "    Group top-level sections (level 0) into batches, including all their children.\n",
    "    Each batch contains a specified number of main sections and all their subsections.\n",
    "    \"\"\"\n",
    "    top_level_sections = [s for s in sections if s.level == 0]\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, len(top_level_sections), batch_size):\n",
    "        batch = top_level_sections[i:i + batch_size]\n",
    "        # For each batch, include the main sections and all their nested children\n",
    "        batch_with_children = []\n",
    "        for main_section in batch:\n",
    "            batch_with_children.extend(flatten_sections([main_section]))\n",
    "        batches.append(batch_with_children)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "# Function to generate plans for a batch of sections\n",
    "def generate_batch_section_plans(\n",
    "    section_batch: List[SectionNode], llm_client, model_name: str, pdf_content: str = \"\"\n",
    ") -> List[SectionPlan]:\n",
    "    \"\"\"\n",
    "    Generate plans for a batch of sections (including main sections and subsections).\n",
    "    \"\"\"\n",
    "    # Create a structured prompt with all sections in the batch\n",
    "    sections_info = []\n",
    "    for section in section_batch:\n",
    "        sections_info.append(f\"- Section {section.section}: {section.title} (Level {section.level})\")\n",
    "    \n",
    "    sections_text = \"\\n\".join(sections_info)\n",
    "    \n",
    "    # Use more PDF content (up to 8000 characters to stay within token limits)\n",
    "    pdf_context = pdf_content[:] if pdf_content else \"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert academic writer helping to create a detailed thesis plan. \n",
    "    Create UNIQUE and SPECIFIC plans for EACH of the following sections and subsections. \n",
    "    \n",
    "    CRITICAL REQUIREMENTS:\n",
    "    1. Each plan must be DIFFERENT and UNIQUE - NO repetition between sections\n",
    "    2. Main sections (level 0) should have broader, strategic plans\n",
    "    3. Subsections (level 1, 2, etc.) should have very specific, focused plans that dive deep into particular aspects\n",
    "    4. Each subsection plan should complement but NOT repeat its parent section\n",
    "    5. Use the PDF context below to make plans more specific and relevant\n",
    "    \n",
    "    Context: This is for an academic thesis about conversational agents in higher education.\n",
    "    \n",
    "    PDF Context (use this to make plans more specific):\n",
    "    {pdf_context}\n",
    "    \n",
    "    Sections to plan (create a UNIQUE plan for EACH one):\n",
    "    {sections_text}\n",
    "    \n",
    "    For EACH section/subsection, provide a comprehensive and UNIQUE plan that includes:\n",
    "    \n",
    "    FOR MAIN SECTIONS (Level 0):\n",
    "    - Broad objectives and strategic goals\n",
    "    - Overall approach and methodology\n",
    "    - Key research questions to address\n",
    "    - Expected major outcomes\n",
    "    - How it fits in the overall thesis narrative\n",
    "    \n",
    "    FOR SUBSECTIONS (Level 1, 2, etc.):\n",
    "    - Very specific objectives focused on one aspect\n",
    "    - Detailed methodologies or approaches specific to this subsection\n",
    "    - Specific data, examples, or case studies to include\n",
    "    - Particular research questions or hypotheses\n",
    "    - Specific deliverables (tables, figures, algorithms, etc.)\n",
    "    - Exact connection to parent section without repeating content\n",
    "    \n",
    "    IMPORTANT: Make each plan distinct and avoid generic language. Be specific about what content, methods, and outcomes are expected for each individual section.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm_client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=BatchSectionPlans,\n",
    "            temperature=0.3,\n",
    "            max_tokens=8192  # Increased token limit for more detailed plans\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.parsed.plans\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating batch plans: {e}\")\n",
    "        return []\n",
    "\n",
    "# Main function to generate plans for all sections in batches\n",
    "def generate_all_section_plans(\n",
    "    sections: List[SectionNode], llm_client, model_name: str, pdf_content: str = \"\", batch_size: int = 3\n",
    ") -> List[SectionPlan]:\n",
    "    \"\"\"\n",
    "    Generate plans for all sections by processing them in batches.\n",
    "    \"\"\"\n",
    "    all_plans = []\n",
    "    \n",
    "    # Group sections into batches\n",
    "    section_batches = group_sections_for_batch_processing(sections, batch_size)\n",
    "    \n",
    "    print(f\"Processing {len(section_batches)} batches...\")\n",
    "    print(f\"PDF content length: {len(pdf_content)} characters\")\n",
    "    \n",
    "    for i, batch in enumerate(section_batches):\n",
    "        print(f\"Processing batch {i+1}/{len(section_batches)} with {len(batch)} sections...\")\n",
    "        \n",
    "        # Show which sections are in this batch\n",
    "        batch_sections = [f\"{s.section}: {s.title} (L{s.level})\" for s in batch]\n",
    "        print(f\"  Sections in batch: {', '.join(batch_sections)}\")\n",
    "        \n",
    "        batch_plans = generate_batch_section_plans(batch, llm_client, model_name, pdf_content)\n",
    "        all_plans.extend(batch_plans)\n",
    "        \n",
    "        print(f\"Generated {len(batch_plans)} plans for batch {i+1}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return all_plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b385eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ccf2ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded PDF with 13752 characters\n",
      "First 500 characters of PDF: 1.1 Motivation and Objectives  \n",
      "Over the last decade, higher -education institutions have adopted a variety of digital \n",
      "tools—ranging from static FAQ pages to rudimentary rule -based chatbots —to \n",
      "streamline administrative processes and enhance service delivery. However, at \n",
      "Friedrich -Alexander -Universität Erlangen -Nürnberg (FAU), students and staff still \n",
      "encounter persistent obstacles when seeki ng information about enrollment \n",
      "procedures, course registration deadlines, examination regulati...\n",
      "Processing 4 batches...\n",
      "PDF content length: 13752 characters\n",
      "Processing batch 1/4 with 26 sections...\n",
      "  Sections in batch: 1: Introduction (L0), 1.1: Motivation and Objectives (L1), 1.2: Thesis Scope and Contributions (L1), 1.3: Thesis Organization (L1), 2: Background and Foundations (L0), 2.1: Domain Background: Conversational Agents in Higher Education (L1), 2.1.1: Evolution and Definitions of Conversational Agents (L2), 2.1.2: Use Cases and Impact in University Administration (L2), 2.2: Technical Foundations: LLM Architectures and Methods (L1), 2.2.1: Transformer Architecture Overview (L2), 2.2.2: Pretraining Paradigms and Domain Adaptation (L2), 2.2.3: Parameter-Efficient Fine-Tuning (LoRA) (L2), 2.2.3.1: LoRA Methodology and Mechanisms (L3), 2.2.3.2: Comparison with Other PEFT Techniques (L3), 2.2.4: Supervised Fine-Tuning vs. Direct Preference Optimization (L2), 2.2.4.1: Supervised Fine-Tuning (SFT) Techniques (L3), 2.2.4.2: Direct Preference Optimization (DPO) Principles (L3), 2.2.4.3: Hybrid SFT + DPO Approaches (L3), 2.2.5: Topic Modeling and Content Filtering Techniques (L2), 2.2.5.1: Unsupervised Topic Modeling (L3), 2.2.5.2: LLM-Based Theme Classification (L3), 3: Related Work (L0), 3.1: Domain-Specific LLMs for Administrative Support (L1), 3.2: Prompt Engineering for User-Aware Generation (L1), 3.3: Evaluation Metrics for Conversational Agents (L1), 3.4: Research Gaps in University Management Chatbots (L1)\n",
      "Processing 4 batches...\n",
      "PDF content length: 13752 characters\n",
      "Processing batch 1/4 with 26 sections...\n",
      "  Sections in batch: 1: Introduction (L0), 1.1: Motivation and Objectives (L1), 1.2: Thesis Scope and Contributions (L1), 1.3: Thesis Organization (L1), 2: Background and Foundations (L0), 2.1: Domain Background: Conversational Agents in Higher Education (L1), 2.1.1: Evolution and Definitions of Conversational Agents (L2), 2.1.2: Use Cases and Impact in University Administration (L2), 2.2: Technical Foundations: LLM Architectures and Methods (L1), 2.2.1: Transformer Architecture Overview (L2), 2.2.2: Pretraining Paradigms and Domain Adaptation (L2), 2.2.3: Parameter-Efficient Fine-Tuning (LoRA) (L2), 2.2.3.1: LoRA Methodology and Mechanisms (L3), 2.2.3.2: Comparison with Other PEFT Techniques (L3), 2.2.4: Supervised Fine-Tuning vs. Direct Preference Optimization (L2), 2.2.4.1: Supervised Fine-Tuning (SFT) Techniques (L3), 2.2.4.2: Direct Preference Optimization (DPO) Principles (L3), 2.2.4.3: Hybrid SFT + DPO Approaches (L3), 2.2.5: Topic Modeling and Content Filtering Techniques (L2), 2.2.5.1: Unsupervised Topic Modeling (L3), 2.2.5.2: LLM-Based Theme Classification (L3), 3: Related Work (L0), 3.1: Domain-Specific LLMs for Administrative Support (L1), 3.2: Prompt Engineering for User-Aware Generation (L1), 3.3: Evaluation Metrics for Conversational Agents (L1), 3.4: Research Gaps in University Management Chatbots (L1)\n",
      "Generated 26 plans for batch 1\n",
      "--------------------------------------------------\n",
      "Processing batch 2/4 with 24 sections...\n",
      "  Sections in batch: 4: Data Acquisition and Preprocessing (L0), 4.1: FAU Administrative Data Harvesting (L1), 4.2: Text Segmentation into 4 000-Token Chunks (L1), 4.3: Initial QA-Pair Generation with Mistral (L1), 4.4: Two-Stage Filtering Pipeline (L1), 4.4.1: Unsupervised Topic Modeling (BERTopic) (L2), 4.4.2: LLM-Based Theme Classification and Lecture Removal (L2), 4.5: Construction of Chosen vs. Rejected Response Sets (L1), 5: Model Fine-Tuning and Prompt Engineering (L0), 5.1: Model Selection: LLaMA 3 and Falcon 7B (L1), 5.1.1: RAG vs. Role-Aware Fine-Tuning: A Design Justification (L2), 5.2: Supervised Fine-Tuning on Management QA Corpus (L1), 5.3: Direct Preference Optimization (DPO) with QLoRA (L1), 5.4: Custom Prompt Templates and Persona Embedding (L1), 5.4.1: Static Persona Injection (User Background & Expert Role) (L2), 5.4.2: Role-Based Persona Modeling (Context-Adaptive Generation) (L2), 5.4.3: Dynamic Context Windows and Memory Traces (L2), 5.5: Implementation Details and Training Infrastructure (L1), 6: Experimental Design and Evaluation (L0), 6.1: Evaluation Metrics: Precision, Recall, F1-Score (L1), 6.2: Baseline vs. Fine-Tuned Model Comparisons (L1), 6.3: User-Role Simulation and Contextual Tests (L1), 6.4: Ablation Study on Prompt Components (L1), 6.5: Statistical Significance and Error Analysis (L1)\n",
      "Generated 26 plans for batch 1\n",
      "--------------------------------------------------\n",
      "Processing batch 2/4 with 24 sections...\n",
      "  Sections in batch: 4: Data Acquisition and Preprocessing (L0), 4.1: FAU Administrative Data Harvesting (L1), 4.2: Text Segmentation into 4 000-Token Chunks (L1), 4.3: Initial QA-Pair Generation with Mistral (L1), 4.4: Two-Stage Filtering Pipeline (L1), 4.4.1: Unsupervised Topic Modeling (BERTopic) (L2), 4.4.2: LLM-Based Theme Classification and Lecture Removal (L2), 4.5: Construction of Chosen vs. Rejected Response Sets (L1), 5: Model Fine-Tuning and Prompt Engineering (L0), 5.1: Model Selection: LLaMA 3 and Falcon 7B (L1), 5.1.1: RAG vs. Role-Aware Fine-Tuning: A Design Justification (L2), 5.2: Supervised Fine-Tuning on Management QA Corpus (L1), 5.3: Direct Preference Optimization (DPO) with QLoRA (L1), 5.4: Custom Prompt Templates and Persona Embedding (L1), 5.4.1: Static Persona Injection (User Background & Expert Role) (L2), 5.4.2: Role-Based Persona Modeling (Context-Adaptive Generation) (L2), 5.4.3: Dynamic Context Windows and Memory Traces (L2), 5.5: Implementation Details and Training Infrastructure (L1), 6: Experimental Design and Evaluation (L0), 6.1: Evaluation Metrics: Precision, Recall, F1-Score (L1), 6.2: Baseline vs. Fine-Tuned Model Comparisons (L1), 6.3: User-Role Simulation and Contextual Tests (L1), 6.4: Ablation Study on Prompt Components (L1), 6.5: Statistical Significance and Error Analysis (L1)\n",
      "Generated 24 plans for batch 2\n",
      "--------------------------------------------------\n",
      "Processing batch 3/4 with 10 sections...\n",
      "  Sections in batch: 7: Results and Discussion (L0), 7.1: Quantitative Performance Gains (+10 % Precision, +12 % F1) (L1), 7.2: Qualitative Case Studies and Exemplars (L1), 7.3: Limitations of the Current Approach (L1), 7.4: Implications for University Administrative Workflows (L1), 8: Conclusion and Future Work (L0), 8.1: Summary of Contributions (L1), 8.2: Recommendations for Deployment (L1), 8.3: Directions for Further Research (L1), : List of Abbreviations (L0)\n",
      "Generated 24 plans for batch 2\n",
      "--------------------------------------------------\n",
      "Processing batch 3/4 with 10 sections...\n",
      "  Sections in batch: 7: Results and Discussion (L0), 7.1: Quantitative Performance Gains (+10 % Precision, +12 % F1) (L1), 7.2: Qualitative Case Studies and Exemplars (L1), 7.3: Limitations of the Current Approach (L1), 7.4: Implications for University Administrative Workflows (L1), 8: Conclusion and Future Work (L0), 8.1: Summary of Contributions (L1), 8.2: Recommendations for Deployment (L1), 8.3: Directions for Further Research (L1), : List of Abbreviations (L0)\n",
      "Generated 10 plans for batch 3\n",
      "--------------------------------------------------\n",
      "Processing batch 4/4 with 3 sections...\n",
      "  Sections in batch: : List of Figures (L0), : List of Tables (L0), : References (L0)\n",
      "Generated 10 plans for batch 3\n",
      "--------------------------------------------------\n",
      "Processing batch 4/4 with 3 sections...\n",
      "  Sections in batch: : List of Figures (L0), : List of Tables (L0), : References (L0)\n",
      "Generated 3 plans for batch 4\n",
      "--------------------------------------------------\n",
      "Generated 63 total plans\n",
      "Section 1: Introduction (Level 0)\n",
      "  Plan length: 943 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Establish the significance of conversational agents in h...\n",
      "\n",
      "Section 1.1: Motivation and Objectives (Level 1)\n",
      "  Plan length: 1004 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Analyze the current administrative challenges at FAU.\n",
      "- Define the role o...\n",
      "\n",
      "Section 1.2: Thesis Scope and Contributions (Level 1)\n",
      "  Plan length: 887 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Define the boundaries of the thesis research.\n",
      "- Highlight the unique cont...\n",
      "\n",
      "Section 1.3: Thesis Organization (Level 1)\n",
      "  Plan length: 836 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Provide a roadmap of the thesis structure.\n",
      "- Guide the reader through the...\n",
      "\n",
      "Section 2: Background and Foundations (Level 0)\n",
      "  Plan length: 866 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Establish the theoretical and technical foundations for ...\n",
      "\n",
      "Section 2.1: Domain Background: Conversational Agents in Higher Education (Level 1)\n",
      "  Plan length: 883 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore the role of conversational agents in university settings.\n",
      "- Analy...\n",
      "\n",
      "Section 2.1.1: Evolution and Definitions of Conversational Agents (Level 2)\n",
      "  Plan length: 843 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Define conversational agents and trace their evolution.\n",
      "- Highlight key m...\n",
      "\n",
      "Section 2.1.2: Use Cases and Impact in University Administration (Level 2)\n",
      "  Plan length: 918 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Investigate specific use cases of conversational agents in university adm...\n",
      "\n",
      "Section 2.2: Technical Foundations: LLM Architectures and Methods (Level 1)\n",
      "  Plan length: 853 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore the technical underpinnings of LLM architectures.\n",
      "- Discuss metho...\n",
      "\n",
      "Section 2.2.1: Transformer Architecture Overview (Level 2)\n",
      "  Plan length: 757 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Provide a detailed overview of the Transformer architecture.\n",
      "- Discuss it...\n",
      "\n",
      "Section 2.2.2: Pretraining Paradigms and Domain Adaptation (Level 2)\n",
      "  Plan length: 783 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore pretraining paradigms relevant to LLMs.\n",
      "- Discuss domain adaptati...\n",
      "\n",
      "Section 2.2.3: Parameter-Efficient Fine-Tuning (LoRA) (Level 2)\n",
      "  Plan length: 759 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Investigate the LoRA methodology for fine-tuning LLMs.\n",
      "- Assess its effic...\n",
      "\n",
      "Section 2.2.3.1: LoRA Methodology and Mechanisms (Level 3)\n",
      "  Plan length: 716 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Detail the mechanisms of the LoRA methodology.\n",
      "- Explain its technical co...\n",
      "\n",
      "Section 2.2.3.2: Comparison with Other PEFT Techniques (Level 3)\n",
      "  Plan length: 768 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Compare LoRA with other PEFT techniques.\n",
      "- Assess their relative advantag...\n",
      "\n",
      "Section 2.2.4: Supervised Fine-Tuning vs. Direct Preference Optimization (Level 2)\n",
      "  Plan length: 756 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Compare supervised fine-tuning and DPO methods.\n",
      "- Assess their effectiven...\n",
      "\n",
      "Section 2.2.4.1: Supervised Fine-Tuning (SFT) Techniques (Level 3)\n",
      "  Plan length: 724 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Detail the techniques of supervised fine-tuning.\n",
      "- Explain their applicat...\n",
      "\n",
      "Section 2.2.4.2: Direct Preference Optimization (DPO) Principles (Level 3)\n",
      "  Plan length: 677 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explain the principles of DPO.\n",
      "- Discuss its applications in LLMs.\n",
      "\n",
      "**Det...\n",
      "\n",
      "Section 2.2.4.3: Hybrid SFT + DPO Approaches (Level 3)\n",
      "  Plan length: 788 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Investigate hybrid approaches combining SFT and DPO.\n",
      "- Assess their effec...\n",
      "\n",
      "Section 2.2.5: Topic Modeling and Content Filtering Techniques (Level 2)\n",
      "  Plan length: 793 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore topic modeling and content filtering techniques.\n",
      "- Assess their r...\n",
      "\n",
      "Section 2.2.5.1: Unsupervised Topic Modeling (Level 3)\n",
      "  Plan length: 789 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Detail unsupervised topic modeling techniques.\n",
      "- Explain their applicatio...\n",
      "\n",
      "Section 2.2.5.2: LLM-Based Theme Classification (Level 3)\n",
      "  Plan length: 807 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explain LLM-based theme classification techniques.\n",
      "- Discuss their applic...\n",
      "\n",
      "Section 3: Related Work (Level 0)\n",
      "  Plan length: 825 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Review existing research on domain-specific LLMs and con...\n",
      "\n",
      "Section 3.1: Domain-Specific LLMs for Administrative Support (Level 1)\n",
      "  Plan length: 848 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore the role of domain-specific LLMs in administrative support.\n",
      "- Ass...\n",
      "\n",
      "Section 3.2: Prompt Engineering for User-Aware Generation (Level 1)\n",
      "  Plan length: 862 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Investigate prompt engineering techniques for user-aware generation.\n",
      "- As...\n",
      "\n",
      "Section 3.3: Evaluation Metrics for Conversational Agents (Level 1)\n",
      "  Plan length: 875 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore evaluation metrics for assessing conversational agents.\n",
      "- Discuss...\n",
      "\n",
      "Section 3.4: Research Gaps in University Management Chatbots (Level 1)\n",
      "  Plan length: 794 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Identify research gaps in university management chatbots.\n",
      "- Discuss oppor...\n",
      "\n",
      "Section 4: Data Acquisition and Preprocessing (Level 0)\n",
      "  Plan length: 890 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Develop a comprehensive pipeline for acquiring, cleaning...\n",
      "\n",
      "Section 4.1: FAU Administrative Data Harvesting (Level 1)\n",
      "  Plan length: 756 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Automate the collection of FAU's administrative documents from multiple s...\n",
      "\n",
      "Section 4.2: Text Segmentation into 4,000-Token Chunks (Level 1)\n",
      "  Plan length: 665 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Segment large documents into manageable chunks for model training.\n",
      "\n",
      "**Det...\n",
      "\n",
      "Section 4.3: Initial QA-Pair Generation with Mistral (Level 1)\n",
      "  Plan length: 661 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Generate synthetic QA pairs from segmented text using Mistral.\n",
      "\n",
      "**Detaile...\n",
      "\n",
      "Section 4.4: Two-Stage Filtering Pipeline (Level 1)\n",
      "  Plan length: 632 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Filter QA pairs to isolate management-focused content.\n",
      "\n",
      "**Detailed Method...\n",
      "\n",
      "Section 4.4.1: Unsupervised Topic Modeling (BERTopic) (Level 2)\n",
      "  Plan length: 608 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Identify and cluster topics within QA pairs using BERTopic.\n",
      "\n",
      "**Detailed M...\n",
      "\n",
      "Section 4.4.2: LLM-Based Theme Classification and Lecture Removal (Level 2)\n",
      "  Plan length: 644 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Classify and remove non-management content using LLMs.\n",
      "\n",
      "**Detailed Method...\n",
      "\n",
      "Section 4.5: Construction of Chosen vs. Rejected Response Sets (Level 1)\n",
      "  Plan length: 622 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Create datasets of preferred and non-preferred responses for model traini...\n",
      "\n",
      "Section 5: Model Fine-Tuning and Prompt Engineering (Level 0)\n",
      "  Plan length: 687 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Enhance model performance through fine-tuning and person...\n",
      "\n",
      "Section 5.1: Model Selection: LLaMA 3 and Falcon 7B (Level 1)\n",
      "  Plan length: 660 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Justify the selection of LLaMA 3 and Falcon 7B for model fine-tuning.\n",
      "\n",
      "**...\n",
      "\n",
      "Section 5.1.1: RAG vs. Role-Aware Fine-Tuning: A Design Justification (Level 2)\n",
      "  Plan length: 665 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Compare RAG and role-aware fine-tuning methodologies.\n",
      "\n",
      "**Detailed Methodo...\n",
      "\n",
      "Section 5.2: Supervised Fine-Tuning on Management QA Corpus (Level 1)\n",
      "  Plan length: 607 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Fine-tune models using the curated management QA corpus.\n",
      "\n",
      "**Detailed Meth...\n",
      "\n",
      "Section 5.3: Direct Preference Optimization (DPO) with QLoRA (Level 1)\n",
      "  Plan length: 614 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Optimize model outputs to align with user preferences using DPO.\n",
      "\n",
      "**Detai...\n",
      "\n",
      "Section 5.4: Custom Prompt Templates and Persona Embedding (Level 1)\n",
      "  Plan length: 655 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Design personalized prompt templates incorporating user and role metadata...\n",
      "\n",
      "Section 5.4.1: Static Persona Injection (User Background & Expert Role) (Level 2)\n",
      "  Plan length: 662 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Embed static persona data into prompts for consistent user-role interacti...\n",
      "\n",
      "Section 5.4.2: Role-Based Persona Modeling (Context-Adaptive Generation) (Level 2)\n",
      "  Plan length: 647 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Develop role-based persona models for context-adaptive response generatio...\n",
      "\n",
      "Section 5.4.3: Dynamic Context Windows and Memory Traces (Level 2)\n",
      "  Plan length: 651 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Implement dynamic context windows and memory traces for enhanced interact...\n",
      "\n",
      "Section 5.5: Implementation Details and Training Infrastructure (Level 1)\n",
      "  Plan length: 701 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Detail the technical infrastructure and implementation specifics for mode...\n",
      "\n",
      "Section 6: Experimental Design and Evaluation (Level 0)\n",
      "  Plan length: 687 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Establish a rigorous evaluation framework for assessing ...\n",
      "\n",
      "Section 6.1: Evaluation Metrics: Precision, Recall, F1-Score (Level 1)\n",
      "  Plan length: 620 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Define and implement key evaluation metrics for model assessment.\n",
      "\n",
      "**Deta...\n",
      "\n",
      "Section 6.2: Baseline vs. Fine-Tuned Model Comparisons (Level 1)\n",
      "  Plan length: 626 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Compare performance of baseline models against fine-tuned versions.\n",
      "\n",
      "**De...\n",
      "\n",
      "Section 6.3: User-Role Simulation and Contextual Tests (Level 1)\n",
      "  Plan length: 636 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Test model performance in simulated user-role scenarios.\n",
      "\n",
      "**Detailed Meth...\n",
      "\n",
      "Section 6.4: Ablation Study on Prompt Components (Level 1)\n",
      "  Plan length: 652 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Analyze the impact of different prompt components on model performance.\n",
      "\n",
      "...\n",
      "\n",
      "Section 6.5: Statistical Significance and Error Analysis (Level 1)\n",
      "  Plan length: 651 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Assess statistical significance of evaluation results and conduct error a...\n",
      "\n",
      "Section 7: Results and Discussion (Level 0)\n",
      "  Plan length: 1415 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- To synthesize the empirical findings from quantitative a...\n",
      "\n",
      "Section 7.1: Quantitative Performance Gains (+10% Precision, +12% F1) (Level 1)\n",
      "  Plan length: 1006 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To quantify the improvements in precision and F1 scores achieved by the c...\n",
      "\n",
      "Section 7.2: Qualitative Case Studies and Exemplars (Level 1)\n",
      "  Plan length: 891 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To explore real-world interactions and user experiences with the conversa...\n",
      "\n",
      "Section 7.3: Limitations of the Current Approach (Level 1)\n",
      "  Plan length: 855 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To critically assess the limitations and challenges faced by the conversa...\n",
      "\n",
      "Section 7.4: Implications for University Administrative Workflows (Level 1)\n",
      "  Plan length: 919 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To explore the practical implications of deploying the conversational age...\n",
      "\n",
      "Section 8: Conclusion and Future Work (Level 0)\n",
      "  Plan length: 808 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- To summarize the key contributions and findings of the t...\n",
      "\n",
      "Section 8.1: Summary of Contributions (Level 1)\n",
      "  Plan length: 742 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To succinctly summarize the key contributions of the thesis.\n",
      "\n",
      "**Detailed ...\n",
      "\n",
      "Section 8.2: Recommendations for Deployment (Level 1)\n",
      "  Plan length: 764 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To provide practical recommendations for deploying the conversational age...\n",
      "\n",
      "Section 8.3: Directions for Further Research (Level 1)\n",
      "  Plan length: 758 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To identify promising avenues for future research and development.\n",
      "\n",
      "**Det...\n",
      "\n",
      "Section List of Abbreviations: List of Abbreviations (Level 0)\n",
      "  Plan length: 783 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- To provide a comprehensive list of abbreviations used th...\n",
      "\n",
      "Section 0: List of Figures (Level 0)\n",
      "  Plan length: 1186 characters\n",
      "  Plan preview: ### Broad Objectives and Strategic Goals\n",
      "- **Objective**: To visually represent complex concepts, me...\n",
      "\n",
      "Section 0: List of Tables (Level 0)\n",
      "  Plan length: 1111 characters\n",
      "  Plan preview: ### Broad Objectives and Strategic Goals\n",
      "- **Objective**: To systematically present data, results, a...\n",
      "\n",
      "Section 0: References (Level 0)\n",
      "  Plan length: 1074 characters\n",
      "  Plan preview: ### Broad Objectives and Strategic Goals\n",
      "- **Objective**: To compile a comprehensive list of all sou...\n",
      "\n",
      "Generated 3 plans for batch 4\n",
      "--------------------------------------------------\n",
      "Generated 63 total plans\n",
      "Section 1: Introduction (Level 0)\n",
      "  Plan length: 943 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Establish the significance of conversational agents in h...\n",
      "\n",
      "Section 1.1: Motivation and Objectives (Level 1)\n",
      "  Plan length: 1004 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Analyze the current administrative challenges at FAU.\n",
      "- Define the role o...\n",
      "\n",
      "Section 1.2: Thesis Scope and Contributions (Level 1)\n",
      "  Plan length: 887 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Define the boundaries of the thesis research.\n",
      "- Highlight the unique cont...\n",
      "\n",
      "Section 1.3: Thesis Organization (Level 1)\n",
      "  Plan length: 836 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Provide a roadmap of the thesis structure.\n",
      "- Guide the reader through the...\n",
      "\n",
      "Section 2: Background and Foundations (Level 0)\n",
      "  Plan length: 866 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Establish the theoretical and technical foundations for ...\n",
      "\n",
      "Section 2.1: Domain Background: Conversational Agents in Higher Education (Level 1)\n",
      "  Plan length: 883 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore the role of conversational agents in university settings.\n",
      "- Analy...\n",
      "\n",
      "Section 2.1.1: Evolution and Definitions of Conversational Agents (Level 2)\n",
      "  Plan length: 843 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Define conversational agents and trace their evolution.\n",
      "- Highlight key m...\n",
      "\n",
      "Section 2.1.2: Use Cases and Impact in University Administration (Level 2)\n",
      "  Plan length: 918 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Investigate specific use cases of conversational agents in university adm...\n",
      "\n",
      "Section 2.2: Technical Foundations: LLM Architectures and Methods (Level 1)\n",
      "  Plan length: 853 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore the technical underpinnings of LLM architectures.\n",
      "- Discuss metho...\n",
      "\n",
      "Section 2.2.1: Transformer Architecture Overview (Level 2)\n",
      "  Plan length: 757 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Provide a detailed overview of the Transformer architecture.\n",
      "- Discuss it...\n",
      "\n",
      "Section 2.2.2: Pretraining Paradigms and Domain Adaptation (Level 2)\n",
      "  Plan length: 783 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore pretraining paradigms relevant to LLMs.\n",
      "- Discuss domain adaptati...\n",
      "\n",
      "Section 2.2.3: Parameter-Efficient Fine-Tuning (LoRA) (Level 2)\n",
      "  Plan length: 759 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Investigate the LoRA methodology for fine-tuning LLMs.\n",
      "- Assess its effic...\n",
      "\n",
      "Section 2.2.3.1: LoRA Methodology and Mechanisms (Level 3)\n",
      "  Plan length: 716 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Detail the mechanisms of the LoRA methodology.\n",
      "- Explain its technical co...\n",
      "\n",
      "Section 2.2.3.2: Comparison with Other PEFT Techniques (Level 3)\n",
      "  Plan length: 768 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Compare LoRA with other PEFT techniques.\n",
      "- Assess their relative advantag...\n",
      "\n",
      "Section 2.2.4: Supervised Fine-Tuning vs. Direct Preference Optimization (Level 2)\n",
      "  Plan length: 756 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Compare supervised fine-tuning and DPO methods.\n",
      "- Assess their effectiven...\n",
      "\n",
      "Section 2.2.4.1: Supervised Fine-Tuning (SFT) Techniques (Level 3)\n",
      "  Plan length: 724 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Detail the techniques of supervised fine-tuning.\n",
      "- Explain their applicat...\n",
      "\n",
      "Section 2.2.4.2: Direct Preference Optimization (DPO) Principles (Level 3)\n",
      "  Plan length: 677 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explain the principles of DPO.\n",
      "- Discuss its applications in LLMs.\n",
      "\n",
      "**Det...\n",
      "\n",
      "Section 2.2.4.3: Hybrid SFT + DPO Approaches (Level 3)\n",
      "  Plan length: 788 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Investigate hybrid approaches combining SFT and DPO.\n",
      "- Assess their effec...\n",
      "\n",
      "Section 2.2.5: Topic Modeling and Content Filtering Techniques (Level 2)\n",
      "  Plan length: 793 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore topic modeling and content filtering techniques.\n",
      "- Assess their r...\n",
      "\n",
      "Section 2.2.5.1: Unsupervised Topic Modeling (Level 3)\n",
      "  Plan length: 789 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Detail unsupervised topic modeling techniques.\n",
      "- Explain their applicatio...\n",
      "\n",
      "Section 2.2.5.2: LLM-Based Theme Classification (Level 3)\n",
      "  Plan length: 807 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explain LLM-based theme classification techniques.\n",
      "- Discuss their applic...\n",
      "\n",
      "Section 3: Related Work (Level 0)\n",
      "  Plan length: 825 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Review existing research on domain-specific LLMs and con...\n",
      "\n",
      "Section 3.1: Domain-Specific LLMs for Administrative Support (Level 1)\n",
      "  Plan length: 848 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore the role of domain-specific LLMs in administrative support.\n",
      "- Ass...\n",
      "\n",
      "Section 3.2: Prompt Engineering for User-Aware Generation (Level 1)\n",
      "  Plan length: 862 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Investigate prompt engineering techniques for user-aware generation.\n",
      "- As...\n",
      "\n",
      "Section 3.3: Evaluation Metrics for Conversational Agents (Level 1)\n",
      "  Plan length: 875 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Explore evaluation metrics for assessing conversational agents.\n",
      "- Discuss...\n",
      "\n",
      "Section 3.4: Research Gaps in University Management Chatbots (Level 1)\n",
      "  Plan length: 794 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Identify research gaps in university management chatbots.\n",
      "- Discuss oppor...\n",
      "\n",
      "Section 4: Data Acquisition and Preprocessing (Level 0)\n",
      "  Plan length: 890 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Develop a comprehensive pipeline for acquiring, cleaning...\n",
      "\n",
      "Section 4.1: FAU Administrative Data Harvesting (Level 1)\n",
      "  Plan length: 756 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Automate the collection of FAU's administrative documents from multiple s...\n",
      "\n",
      "Section 4.2: Text Segmentation into 4,000-Token Chunks (Level 1)\n",
      "  Plan length: 665 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Segment large documents into manageable chunks for model training.\n",
      "\n",
      "**Det...\n",
      "\n",
      "Section 4.3: Initial QA-Pair Generation with Mistral (Level 1)\n",
      "  Plan length: 661 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Generate synthetic QA pairs from segmented text using Mistral.\n",
      "\n",
      "**Detaile...\n",
      "\n",
      "Section 4.4: Two-Stage Filtering Pipeline (Level 1)\n",
      "  Plan length: 632 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Filter QA pairs to isolate management-focused content.\n",
      "\n",
      "**Detailed Method...\n",
      "\n",
      "Section 4.4.1: Unsupervised Topic Modeling (BERTopic) (Level 2)\n",
      "  Plan length: 608 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Identify and cluster topics within QA pairs using BERTopic.\n",
      "\n",
      "**Detailed M...\n",
      "\n",
      "Section 4.4.2: LLM-Based Theme Classification and Lecture Removal (Level 2)\n",
      "  Plan length: 644 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Classify and remove non-management content using LLMs.\n",
      "\n",
      "**Detailed Method...\n",
      "\n",
      "Section 4.5: Construction of Chosen vs. Rejected Response Sets (Level 1)\n",
      "  Plan length: 622 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Create datasets of preferred and non-preferred responses for model traini...\n",
      "\n",
      "Section 5: Model Fine-Tuning and Prompt Engineering (Level 0)\n",
      "  Plan length: 687 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Enhance model performance through fine-tuning and person...\n",
      "\n",
      "Section 5.1: Model Selection: LLaMA 3 and Falcon 7B (Level 1)\n",
      "  Plan length: 660 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Justify the selection of LLaMA 3 and Falcon 7B for model fine-tuning.\n",
      "\n",
      "**...\n",
      "\n",
      "Section 5.1.1: RAG vs. Role-Aware Fine-Tuning: A Design Justification (Level 2)\n",
      "  Plan length: 665 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Compare RAG and role-aware fine-tuning methodologies.\n",
      "\n",
      "**Detailed Methodo...\n",
      "\n",
      "Section 5.2: Supervised Fine-Tuning on Management QA Corpus (Level 1)\n",
      "  Plan length: 607 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Fine-tune models using the curated management QA corpus.\n",
      "\n",
      "**Detailed Meth...\n",
      "\n",
      "Section 5.3: Direct Preference Optimization (DPO) with QLoRA (Level 1)\n",
      "  Plan length: 614 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Optimize model outputs to align with user preferences using DPO.\n",
      "\n",
      "**Detai...\n",
      "\n",
      "Section 5.4: Custom Prompt Templates and Persona Embedding (Level 1)\n",
      "  Plan length: 655 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Design personalized prompt templates incorporating user and role metadata...\n",
      "\n",
      "Section 5.4.1: Static Persona Injection (User Background & Expert Role) (Level 2)\n",
      "  Plan length: 662 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Embed static persona data into prompts for consistent user-role interacti...\n",
      "\n",
      "Section 5.4.2: Role-Based Persona Modeling (Context-Adaptive Generation) (Level 2)\n",
      "  Plan length: 647 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Develop role-based persona models for context-adaptive response generatio...\n",
      "\n",
      "Section 5.4.3: Dynamic Context Windows and Memory Traces (Level 2)\n",
      "  Plan length: 651 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Implement dynamic context windows and memory traces for enhanced interact...\n",
      "\n",
      "Section 5.5: Implementation Details and Training Infrastructure (Level 1)\n",
      "  Plan length: 701 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Detail the technical infrastructure and implementation specifics for mode...\n",
      "\n",
      "Section 6: Experimental Design and Evaluation (Level 0)\n",
      "  Plan length: 687 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- Establish a rigorous evaluation framework for assessing ...\n",
      "\n",
      "Section 6.1: Evaluation Metrics: Precision, Recall, F1-Score (Level 1)\n",
      "  Plan length: 620 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Define and implement key evaluation metrics for model assessment.\n",
      "\n",
      "**Deta...\n",
      "\n",
      "Section 6.2: Baseline vs. Fine-Tuned Model Comparisons (Level 1)\n",
      "  Plan length: 626 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Compare performance of baseline models against fine-tuned versions.\n",
      "\n",
      "**De...\n",
      "\n",
      "Section 6.3: User-Role Simulation and Contextual Tests (Level 1)\n",
      "  Plan length: 636 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Test model performance in simulated user-role scenarios.\n",
      "\n",
      "**Detailed Meth...\n",
      "\n",
      "Section 6.4: Ablation Study on Prompt Components (Level 1)\n",
      "  Plan length: 652 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Analyze the impact of different prompt components on model performance.\n",
      "\n",
      "...\n",
      "\n",
      "Section 6.5: Statistical Significance and Error Analysis (Level 1)\n",
      "  Plan length: 651 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- Assess statistical significance of evaluation results and conduct error a...\n",
      "\n",
      "Section 7: Results and Discussion (Level 0)\n",
      "  Plan length: 1415 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- To synthesize the empirical findings from quantitative a...\n",
      "\n",
      "Section 7.1: Quantitative Performance Gains (+10% Precision, +12% F1) (Level 1)\n",
      "  Plan length: 1006 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To quantify the improvements in precision and F1 scores achieved by the c...\n",
      "\n",
      "Section 7.2: Qualitative Case Studies and Exemplars (Level 1)\n",
      "  Plan length: 891 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To explore real-world interactions and user experiences with the conversa...\n",
      "\n",
      "Section 7.3: Limitations of the Current Approach (Level 1)\n",
      "  Plan length: 855 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To critically assess the limitations and challenges faced by the conversa...\n",
      "\n",
      "Section 7.4: Implications for University Administrative Workflows (Level 1)\n",
      "  Plan length: 919 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To explore the practical implications of deploying the conversational age...\n",
      "\n",
      "Section 8: Conclusion and Future Work (Level 0)\n",
      "  Plan length: 808 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- To summarize the key contributions and findings of the t...\n",
      "\n",
      "Section 8.1: Summary of Contributions (Level 1)\n",
      "  Plan length: 742 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To succinctly summarize the key contributions of the thesis.\n",
      "\n",
      "**Detailed ...\n",
      "\n",
      "Section 8.2: Recommendations for Deployment (Level 1)\n",
      "  Plan length: 764 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To provide practical recommendations for deploying the conversational age...\n",
      "\n",
      "Section 8.3: Directions for Further Research (Level 1)\n",
      "  Plan length: 758 characters\n",
      "  Plan preview: **Specific Objectives:**\n",
      "- To identify promising avenues for future research and development.\n",
      "\n",
      "**Det...\n",
      "\n",
      "Section List of Abbreviations: List of Abbreviations (Level 0)\n",
      "  Plan length: 783 characters\n",
      "  Plan preview: **Broad Objectives and Strategic Goals:**\n",
      "- To provide a comprehensive list of abbreviations used th...\n",
      "\n",
      "Section 0: List of Figures (Level 0)\n",
      "  Plan length: 1186 characters\n",
      "  Plan preview: ### Broad Objectives and Strategic Goals\n",
      "- **Objective**: To visually represent complex concepts, me...\n",
      "\n",
      "Section 0: List of Tables (Level 0)\n",
      "  Plan length: 1111 characters\n",
      "  Plan preview: ### Broad Objectives and Strategic Goals\n",
      "- **Objective**: To systematically present data, results, a...\n",
      "\n",
      "Section 0: References (Level 0)\n",
      "  Plan length: 1074 characters\n",
      "  Plan preview: ### Broad Objectives and Strategic Goals\n",
      "- **Objective**: To compile a comprehensive list of all sou...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call the function to generate plans\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the section hierarchy from the JSON file\n",
    "    sections = read_section_hierarchy(\"section_hierarchy.json\")\n",
    "    \n",
    "    # Read the PDF content (if needed)\n",
    "    try:\n",
    "        pdf_content = read_pdf_content(\"inital_info.pdf\")  # Replace with your actual PDF file path\n",
    "        print(f\"Successfully loaded PDF with {len(pdf_content)} characters\")\n",
    "        print(f\"First 500 characters of PDF: {pdf_content[:500]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read PDF: {e}\")\n",
    "        pdf_content = \"\"\n",
    "    \n",
    "\n",
    "    # Generate plans for all sections in batches of 3 main sections\n",
    "    all_section_plans = generate_all_section_plans(\n",
    "        sections, llm_client, model_name, pdf_content, batch_size=3\n",
    "    )\n",
    "\n",
    "    # Print out the plans as JSON\n",
    "    print(f\"Generated {len(all_section_plans)} total plans\")\n",
    "    # print(json.dumps([plan.model_dump() for plan in all_section_plans], indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Print a summary of the plans generated\n",
    "    for plan in all_section_plans:\n",
    "        print(f\"Section {plan.section_number}: {plan.title} (Level {plan.level})\")\n",
    "        print(f\"  Plan length: {len(plan.plan)} characters\")\n",
    "        print(f\"  Plan preview: {plan.plan[:100]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0482599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the plans to a file\n",
    "with open(\"section_plans.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([plan.model_dump() for plan in all_section_plans], f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0864a383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YAYT\\AppData\\Local\\Temp\\ipykernel_12708\\3926822292.py:16: PydanticDeprecatedSince20: The `update_forward_refs` method is deprecated; use `model_rebuild` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  SectionNode.update_forward_refs()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 root sections; 63 plans; 7 PDF pages.\n",
      "Extracting bullet points for all sections in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/63 [00:12<13:07, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 1 - Introduction: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/63 [00:18<08:53,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 1.1 - Motivation and Objectives: 53 bullets\n",
      "[DONE] Section 1.2 - Thesis Scope and Contributions: 56 bullets\n",
      "[DONE] Section 1.3 - Thesis Organization: 52 bullets\n",
      "[DONE] Section 2 - Background and Foundations: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 6/63 [00:28<03:41,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.1 - Domain Background: Conversational Agents in Higher Education: 56 bullets\n",
      "[DONE] Section 2.1.1 - Evolution and Definitions of Conversational Agents: 50 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 8/63 [00:31<02:50,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.1.2 - Use Cases and Impact in University Administration: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 9/63 [00:34<02:44,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.2 - Technical Foundations: LLM Architectures and Methods: 55 bullets\n",
      "[DONE] Section 2.2.1 - Transformer Architecture Overview: 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 11/63 [00:40<02:35,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.2.2 - Pretraining Paradigms and Domain Adaptation: 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 12/63 [00:45<02:54,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.2.3 - Parameter-Efficient Fine-Tuning (LoRA): 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 13/63 [00:47<02:37,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.2.3.1 - LoRA Methodology and Mechanisms: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 14/63 [00:48<02:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.2.3.2 - Comparison with Other PEFT Techniques: 54 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 15/63 [00:53<02:30,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.2.4 - Supervised Fine-Tuning vs. Direct Preference Optimization: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 16/63 [00:57<02:38,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.2.4.1 - Supervised Fine-Tuning (SFT) Techniques: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 17/63 [00:59<02:24,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.2.4.2 - Direct Preference Optimization (DPO) Principles: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 18/63 [01:02<02:20,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.2.4.3 - Hybrid SFT + DPO Approaches: 55 bullets\n",
      "[DONE] Section 2.2.5 - Topic Modeling and Content Filtering Techniques: 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 20/63 [01:11<02:41,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.2.5.1 - Unsupervised Topic Modeling: 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 21/63 [01:12<02:10,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 2.2.5.2 - LLM-Based Theme Classification: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 22/63 [01:15<02:01,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 3 - Related Work: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 23/63 [01:16<01:41,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 3.1 - Domain-Specific LLMs for Administrative Support: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 24/63 [01:18<01:30,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 3.2 - Prompt Engineering for User-Aware Generation: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 25/63 [01:27<02:35,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 3.3 - Evaluation Metrics for Conversational Agents: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 26/63 [01:28<02:02,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 3.4 - Research Gaps in University Management Chatbots: 54 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 27/63 [01:30<01:47,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 4 - Data Acquisition and Preprocessing: 49 bullets\n",
      "[DONE] Section 4.1 - FAU Administrative Data Harvesting: 48 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 29/63 [01:31<01:01,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 4.2 - Text Segmentation into 4 000-Token Chunks: 50 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 30/63 [01:42<02:10,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 4.3 - Initial QA-Pair Generation with Mistral: 51 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 31/63 [01:43<01:47,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 4.4 - Two-Stage Filtering Pipeline: 44 bullets\n",
      "[DONE] Section 4.4.1 - Unsupervised Topic Modeling (BERTopic): 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 33/63 [01:45<01:10,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 4.4.2 - LLM-Based Theme Classification and Lecture Removal: 53 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 34/63 [01:47<01:06,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 4.5 - Construction of Chosen vs. Rejected Response Sets: 47 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 35/63 [01:55<01:39,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 5 - Model Fine-Tuning and Prompt Engineering: 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 36/63 [01:59<01:40,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 5.1 - Model Selection: LLaMA 3 and Falcon 7B: 56 bullets\n",
      "[DONE] Section 5.1.1 - RAG vs. Role-Aware Fine-Tuning: A Design Justification: 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 38/63 [01:59<00:54,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 5.2 - Supervised Fine-Tuning on Management QA Corpus: 54 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 39/63 [02:04<01:07,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 5.3 - Direct Preference Optimization (DPO) with QLoRA: 51 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 40/63 [02:08<01:13,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 5.4 - Custom Prompt Templates and Persona Embedding: 51 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 41/63 [02:12<01:14,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 5.4.1 - Static Persona Injection (User Background & Expert Role): 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 42/63 [02:13<00:53,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 5.4.2 - Role-Based Persona Modeling (Context-Adaptive Generation): 54 bullets\n",
      "[DONE] Section 5.4.3 - Dynamic Context Windows and Memory Traces: 48 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 44/63 [02:19<00:54,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 5.5 - Implementation Details and Training Infrastructure: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 45/63 [02:24<00:58,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 6 - Experimental Design and Evaluation: 54 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 46/63 [02:27<00:54,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 6.1 - Evaluation Metrics: Precision, Recall, F1-Score: 54 bullets\n",
      "[DONE] Section 6.2 - Baseline vs. Fine-Tuned Model Comparisons: 54 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 48/63 [02:27<00:28,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 6.3 - User-Role Simulation and Contextual Tests: 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 49/63 [02:33<00:38,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 6.4 - Ablation Study on Prompt Components: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 50/63 [02:38<00:43,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 6.5 - Statistical Significance and Error Analysis: 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 51/63 [02:40<00:36,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 7 - Results and Discussion: 56 bullets\n",
      "[DONE] Section 7.1 - Quantitative Performance Gains (+10 % Precision, +12 % F1): 41 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 53/63 [02:41<00:18,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 7.2 - Qualitative Case Studies and Exemplars: 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 54/63 [02:45<00:21,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 7.3 - Limitations of the Current Approach: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 55/63 [02:52<00:29,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 7.4 - Implications for University Administrative Workflows: 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 56/63 [02:55<00:24,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 8 - Conclusion and Future Work: 56 bullets\n",
      "[DONE] Section 8.1 - Summary of Contributions: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 58/63 [02:57<00:12,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 8.2 - Recommendations for Deployment: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 59/63 [02:59<00:09,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section 8.3 - Directions for Further Research: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 60/63 [03:05<00:09,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section  - List of Abbreviations: 55 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 61/63 [03:08<00:06,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section  - List of Figures: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 62/63 [03:09<00:02,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section  - List of Tables: 56 bullets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [03:12<00:00,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Section  - References: 56 bullets\n",
      "Exported: section_hierarchy_with_bullets.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "from PyPDF2 import PdfReader\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SectionNode(BaseModel):\n",
    "    section: str\n",
    "    title: str\n",
    "    level: int\n",
    "    children: List[\"SectionNode\"] = []\n",
    "    bullet_points: Optional[List[str]] = None\n",
    "    plan: Optional[str] = None\n",
    "\n",
    "SectionNode.update_forward_refs()\n",
    "\n",
    "class SectionPlan(BaseModel):\n",
    "    section_number: str\n",
    "    title: str\n",
    "    level: int\n",
    "    plan: str\n",
    "\n",
    "class BulletPointsResponse(BaseModel):\n",
    "    bullet_points: List[str]\n",
    "\n",
    "def read_pdf_pages(path: str) -> List[str]:\n",
    "    reader = PdfReader(path)\n",
    "    return [page.extract_text() or \"\" for page in reader.pages]\n",
    "\n",
    "def load_section_hierarchy(path: str) -> List[SectionNode]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return [SectionNode(**s) for s in data]\n",
    "\n",
    "def load_section_plans(path: str) -> List[SectionPlan]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return [SectionPlan(**s) for s in data]\n",
    "\n",
    "def plans_map_by_number(plans: List[SectionPlan]):\n",
    "    return {p.section_number: p for p in plans}\n",
    "\n",
    "def flatten_sections(sections: List[SectionNode]) -> List[SectionNode]:\n",
    "    result = []\n",
    "    def rec(nodes):\n",
    "        for n in nodes:\n",
    "            result.append(n)\n",
    "            if n.children:\n",
    "                rec(n.children)\n",
    "    rec(sections)\n",
    "    return result\n",
    "\n",
    "def extract_bullet_points_for_section(\n",
    "    section: SectionNode,\n",
    "    plan: Optional[SectionPlan],\n",
    "    pages: List[str],\n",
    "    llm_client,\n",
    "    model_name: str,\n",
    "    bullets_per_section: int = 8\n",
    ") -> List[str]:\n",
    "    all_bullets = []\n",
    "    for page_num, page_text in enumerate(pages, start=1):\n",
    "        prompt = f\"\"\"\n",
    "You are given a section context and a single page of a PDF.\n",
    "\n",
    "Section: {section.section} - {section.title} (Level {section.level})\n",
    "\n",
    "Section Plan (use this as focus guide):\n",
    "{plan.plan if plan else ''}\n",
    "\n",
    "Page Number: {page_num}\n",
    "\n",
    "Page Text:\n",
    "{page_text}\n",
    "\n",
    "Extract up to {bullets_per_section} concise, actionable bullet points strictly relevant to this section and plan.\n",
    "Return only a list of bullet points.\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = llm_client.beta.chat.completions.parse(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                response_format=BulletPointsResponse,\n",
    "                temperature=0.3,\n",
    "                max_tokens=1024\n",
    "            )\n",
    "            for bp in response.choices[0].message.parsed.bullet_points:\n",
    "                if bp not in all_bullets:\n",
    "                    all_bullets.append(bp)\n",
    "        except Exception as e:\n",
    "            print(f\"Error on section {section.section}, page {page_num}: {e}\")\n",
    "    return all_bullets\n",
    "\n",
    "def main():\n",
    "    # --- Load inputs ---\n",
    "    hierarchy = load_section_hierarchy(\"section_hierarchy.json\")\n",
    "    plans = load_section_plans(\"section_plans.json\")\n",
    "    plans_map = plans_map_by_number(plans)\n",
    "    pages = read_pdf_pages(\"initial_info.pdf\")\n",
    "    print(f\"Loaded {len(hierarchy)} root sections; {len(plans)} plans; {len(pages)} PDF pages.\")\n",
    "\n",
    "    # --- Flatten all sections (including children) ---\n",
    "    flat_sections = flatten_sections(hierarchy)\n",
    "\n",
    "    # --- Parallel bullet extraction ---\n",
    "    results_map = {}  # section_number: (plan, bullets)\n",
    "\n",
    "    print(\"Extracting bullet points for all sections in parallel...\")\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = []\n",
    "        for section in flat_sections:\n",
    "            plan = plans_map.get(section.section, None)\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    extract_bullet_points_for_section,\n",
    "                    section, plan, pages, llm_client, model_name\n",
    "                )\n",
    "            )\n",
    "        for section, future in tqdm(zip(flat_sections, futures), total=len(flat_sections)):\n",
    "            try:\n",
    "                bullets = future.result()\n",
    "                results_map[section.section] = {\n",
    "                    \"plan\": plans_map[section.section].plan if section.section in plans_map else None,\n",
    "                    \"bullet_points\": bullets,\n",
    "                }\n",
    "                print(f\"[DONE] Section {section.section} - {section.title}: {len(bullets)} bullets\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Section {section.section} - {section.title}: {e}\")\n",
    "\n",
    "    # --- Attach results back into the hierarchy tree ---\n",
    "    def attach_results(nodes):\n",
    "        for node in nodes:\n",
    "            res = results_map.get(node.section, None)\n",
    "            if res:\n",
    "                node.plan = res[\"plan\"]\n",
    "                node.bullet_points = res[\"bullet_points\"]\n",
    "            if node.children:\n",
    "                attach_results(node.children)\n",
    "    attach_results(hierarchy)\n",
    "\n",
    "    # --- Export enriched tree ---\n",
    "    with open(\"section_hierarchy_with_bullets.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([s.model_dump() for s in hierarchy], f, indent=2, ensure_ascii=False)\n",
    "    print(\"Exported: section_hierarchy_with_bullets.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39d9489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YAYT\\AppData\\Local\\Temp\\ipykernel_12708\\854873559.py:18: PydanticDeprecatedSince20: The `update_forward_refs` method is deprecated; use `model_rebuild` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  SectionNode.update_forward_refs()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drafting section 1 - Introduction\n",
      "Drafting section 1.1 - Motivation and Objectives\n",
      "Drafting section 1.2 - Thesis Scope and Contributions\n",
      "Drafting section 1.3 - Thesis Organization\n",
      "Drafting section 2 - Background and Foundations\n",
      "Drafting section 2.1 - Domain Background: Conversational Agents in Higher Education\n",
      "Drafting section 2.1.1 - Evolution and Definitions of Conversational Agents\n",
      "Drafting section 2.1.2 - Use Cases and Impact in University Administration\n",
      "Drafting section 2.2 - Technical Foundations: LLM Architectures and Methods\n",
      "Drafting section 2.2.1 - Transformer Architecture Overview\n",
      "Drafting section 2.2.2 - Pretraining Paradigms and Domain Adaptation\n",
      "Drafting section 2.2.3 - Parameter-Efficient Fine-Tuning (LoRA)\n",
      "Drafting section 2.2.3.1 - LoRA Methodology and Mechanisms\n",
      "Drafting section 2.2.3.2 - Comparison with Other PEFT Techniques\n",
      "Drafting section 2.2.4 - Supervised Fine-Tuning vs. Direct Preference Optimization\n",
      "Drafting section 2.2.4.1 - Supervised Fine-Tuning (SFT) Techniques\n",
      "Drafting section 2.2.4.2 - Direct Preference Optimization (DPO) Principles\n",
      "Drafting section 2.2.4.3 - Hybrid SFT + DPO Approaches\n",
      "Drafting section 2.2.5 - Topic Modeling and Content Filtering Techniques\n",
      "Drafting section 2.2.5.1 - Unsupervised Topic Modeling\n",
      "Drafting section 2.2.5.2 - LLM-Based Theme Classification\n",
      "Drafting section 3 - Related Work\n",
      "Drafting section 3.1 - Domain-Specific LLMs for Administrative Support\n",
      "Drafting section 3.2 - Prompt Engineering for User-Aware Generation\n",
      "Drafting section 3.3 - Evaluation Metrics for Conversational Agents\n",
      "Drafting section 3.4 - Research Gaps in University Management Chatbots\n",
      "Drafting section 4 - Data Acquisition and Preprocessing\n",
      "Drafting section 4.1 - FAU Administrative Data Harvesting\n",
      "Drafting section 4.2 - Text Segmentation into 4 000-Token Chunks\n",
      "Drafting section 4.3 - Initial QA-Pair Generation with Mistral\n",
      "Drafting section 4.4 - Two-Stage Filtering Pipeline\n",
      "Drafting section 4.4.1 - Unsupervised Topic Modeling (BERTopic)\n",
      "Drafting section 4.4.2 - LLM-Based Theme Classification and Lecture Removal\n",
      "Drafting section 4.5 - Construction of Chosen vs. Rejected Response Sets\n",
      "Drafting section 5 - Model Fine-Tuning and Prompt Engineering\n",
      "Drafting section 5.1 - Model Selection: LLaMA 3 and Falcon 7B\n",
      "Drafting section 5.1.1 - RAG vs. Role-Aware Fine-Tuning: A Design Justification\n",
      "Drafting section 5.2 - Supervised Fine-Tuning on Management QA Corpus\n",
      "Drafting section 5.3 - Direct Preference Optimization (DPO) with QLoRA\n",
      "Drafting section 5.4 - Custom Prompt Templates and Persona Embedding\n",
      "Drafting section 5.4.1 - Static Persona Injection (User Background & Expert Role)\n",
      "Drafting section 5.4.2 - Role-Based Persona Modeling (Context-Adaptive Generation)\n",
      "Drafting section 5.4.3 - Dynamic Context Windows and Memory Traces\n",
      "Drafting section 5.5 - Implementation Details and Training Infrastructure\n",
      "Drafting section 6 - Experimental Design and Evaluation\n",
      "Drafting section 6.1 - Evaluation Metrics: Precision, Recall, F1-Score\n",
      "Drafting section 6.2 - Baseline vs. Fine-Tuned Model Comparisons\n",
      "Drafting section 6.3 - User-Role Simulation and Contextual Tests\n",
      "Drafting section 6.4 - Ablation Study on Prompt Components\n",
      "Drafting section 6.5 - Statistical Significance and Error Analysis\n",
      "Drafting section 7 - Results and Discussion\n",
      "Drafting section 7.1 - Quantitative Performance Gains (+10 % Precision, +12 % F1)\n",
      "Drafting section 7.2 - Qualitative Case Studies and Exemplars\n",
      "Drafting section 7.3 - Limitations of the Current Approach\n",
      "Drafting section 7.4 - Implications for University Administrative Workflows\n",
      "Drafting section 8 - Conclusion and Future Work\n",
      "Drafting section 8.1 - Summary of Contributions\n",
      "Drafting section 8.2 - Recommendations for Deployment\n",
      "Drafting section 8.3 - Directions for Further Research\n",
      "Warning: No plan found for section \n",
      "Warning: No plan found for section \n",
      "Warning: No plan found for section \n",
      "Warning: No plan found for section \n",
      "Drafts generated and saved to section_hierarchy_with_drafts.json\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Section Drafting Pipeline (Hierarchy + Plans -> Drafts with Child Context)\n",
    "# ===============================\n",
    "import json\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# ===============================\n",
    "# 1. MODELS (recursive hierarchy)\n",
    "# ===============================\n",
    "class SectionNode(BaseModel):\n",
    "    section: str\n",
    "    title: str\n",
    "    level: int\n",
    "    children: List[\"SectionNode\"] = []\n",
    "    draft_content: Optional[str] = None\n",
    "\n",
    "SectionNode.update_forward_refs()\n",
    "\n",
    "class SectionPlan(BaseModel):\n",
    "    section_number: str\n",
    "    plan: str\n",
    "\n",
    "class DraftResponse(BaseModel):\n",
    "    draft: str\n",
    "\n",
    "# ===============================\n",
    "# 2. HELPERS (load/save)\n",
    "# ===============================\n",
    "\n",
    "def load_hierarchy(path: str) -> List[SectionNode]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return [SectionNode(**s) for s in data]\n",
    "\n",
    "\n",
    "def load_plans(path: str) -> dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "    return {p[\"section_number\"]: p[\"plan\"] for p in raw}\n",
    "\n",
    "\n",
    "def save_hierarchy(path: str, hierarchy: List[SectionNode]):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([s.model_dump() for s in hierarchy], f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ===============================\n",
    "# 3. Draft Generation for Each Section\n",
    "# ===============================\n",
    "\n",
    "def generate_draft_for_section(\n",
    "    section: SectionNode,\n",
    "    plans: dict,\n",
    "    llm_client,\n",
    "    draft_model_name: str\n",
    ") -> str:\n",
    "    # Main section plan\n",
    "    main_plan = plans.get(section.section, \"\")\n",
    "    # Gather child headings and their plans\n",
    "    child_lines = []\n",
    "    for child in section.children:\n",
    "        cp = plans.get(child.section, \"\")\n",
    "        child_lines.append(f\"Subsection {child.section}: {child.title}\\nPlan: {cp}\")\n",
    "    child_context = \"\\n\\n\".join(child_lines) if child_lines else \"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a meticulous academic writer drafting a thesis section. Write in a clear, human-like tone, ensure originality (no plagiarism), and maintain academic rigor.\n",
    "\n",
    "SECTION CONTEXT\n",
    "---------------\n",
    "Section {section.section}: {section.title}\n",
    "Plan: {main_plan}\n",
    "\n",
    "\"\"\"\n",
    "    if child_context:\n",
    "        prompt += f\"\"\"\n",
    "SUBSECTION CONTEXT\n",
    "------------------\n",
    "{child_context}\n",
    "\n",
    "\"\"\"\n",
    "    prompt += f\"\"\"\n",
    "INSTRUCTIONS\n",
    "------------\n",
    "Based on the section and its subsections' plans above, write coherent, concise draft paragraph(s) covering the main plan and reflecting the structure implied by the subsections. Use academic style, human tone, and avoid copying verbatim from sources.\n",
    "\n",
    "Return only the draft text.\n",
    "Remeber based on the section and the content you are writing adjust the size of the draft and the tone, sna dmek it perfect liek a final version.\n",
    "\"\"\"\n",
    "\n",
    "    response = llm_client.beta.chat.completions.parse(\n",
    "        model=draft_model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format=DraftResponse,\n",
    "        temperature=0.3,\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    return response.choices[0].message.parsed.draft\n",
    "\n",
    "# ===============================\n",
    "# 4. Recursive Draft Enrichment\n",
    "# ===============================\n",
    "\n",
    "def enrich_with_drafts(\n",
    "    nodes: List[SectionNode],\n",
    "    plans: dict,\n",
    "    llm_client,\n",
    "    draft_model_name: str\n",
    "):\n",
    "    for node in nodes:\n",
    "        if node.section in plans:\n",
    "            print(f\"Drafting section {node.section} - {node.title}\")\n",
    "            node.draft_content = generate_draft_for_section(\n",
    "                node, plans, llm_client, draft_model_name\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Warning: No plan found for section {node.section}\")\n",
    "        if node.children:\n",
    "            enrich_with_drafts(node.children, plans, llm_client, draft_model_name)\n",
    "\n",
    "# ===============================\n",
    "# 5. Main Execution\n",
    "# ===============================\n",
    "\n",
    "def main():\n",
    "    hierarchy = load_hierarchy(\"section_hierarchy.json\")\n",
    "    plans = load_plans(\"section_plans.json\")\n",
    "\n",
    "    enrich_with_drafts(hierarchy, plans, llm_client, model_name)\n",
    "\n",
    "    save_hierarchy(\"section_hierarchy_with_drafts.json\", hierarchy)\n",
    "    print(\"Drafts generated and saved to section_hierarchy_with_drafts.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17862ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YAYT\\AppData\\Local\\Temp\\ipykernel_12708\\1315602683.py:29: PydanticDeprecatedSince20: The `update_forward_refs` method is deprecated; use `model_rebuild` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  SectionNode.update_forward_refs()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading section hierarchy with drafts...\n",
      "Loaded 12 root sections\n",
      "\n",
      "Starting research enhancement pipeline...\n",
      "\n",
      "============================================================\n",
      "Processing: 1 - Introduction\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The introduction makes claims about the effectiveness of conversational agents and language models in administrative processes, which require empirical evidence and technical citations.\n",
      "Search queries: ['conversational agents in higher education research papers', 'language models administrative efficiency empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents in higher education research papers\n",
      "Search needed: True\n",
      "Reasoning: The introduction makes claims about the effectiveness of conversational agents and language models in administrative processes, which require empirical evidence and technical citations.\n",
      "Search queries: ['conversational agents in higher education research papers', 'language models administrative efficiency empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents in higher education research papers\n",
      "  Searching: language models administrative efficiency empirical studies\n",
      "  Searching: language models administrative efficiency empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 1.1 - Motivation and Objectives\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 1.1 - Motivation and Objectives\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses the application of specific large language models (GPT-3 and LLaMA-3) in administrative tasks, requiring technical citations and empirical evidence to support claims about their effectiveness.\n",
      "Search queries: ['GPT-3 and LLaMA-3 in administrative tasks research papers', 'large language models in university administration scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: GPT-3 and LLaMA-3 in administrative tasks research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses the application of specific large language models (GPT-3 and LLaMA-3) in administrative tasks, requiring technical citations and empirical evidence to support claims about their effectiveness.\n",
      "Search queries: ['GPT-3 and LLaMA-3 in administrative tasks research papers', 'large language models in university administration scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: GPT-3 and LLaMA-3 in administrative tasks research papers\n",
      "  Searching: large language models in university administration scholarly articles\n",
      "  Searching: large language models in university administration scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 1.2 - Thesis Scope and Contributions\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 1.2 - Thesis Scope and Contributions\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical methods and claims about performance that require academic references to support the thesis scope and contributions.\n",
      "Search queries: ['parameter-efficient fine-tuning methods in conversational agents research papers', 'evaluation metrics for conversational agents in higher education scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: parameter-efficient fine-tuning methods in conversational agents research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical methods and claims about performance that require academic references to support the thesis scope and contributions.\n",
      "Search queries: ['parameter-efficient fine-tuning methods in conversational agents research papers', 'evaluation metrics for conversational agents in higher education scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: parameter-efficient fine-tuning methods in conversational agents research papers\n",
      "  Searching: evaluation metrics for conversational agents in higher education scholarly articles\n",
      "  Searching: evaluation metrics for conversational agents in higher education scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 1\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 1.3 - Thesis Organization\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 1\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 1.3 - Thesis Organization\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: False\n",
      "Reasoning: This section is primarily descriptive of the thesis organization and does not make specific technical or empirical claims that require external citations.\n",
      "No search needed - keeping original draft\n",
      "\n",
      "============================================================\n",
      "Processing: 2 - Background and Foundations\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: False\n",
      "Reasoning: This section is primarily descriptive of the thesis organization and does not make specific technical or empirical claims that require external citations.\n",
      "No search needed - keeping original draft\n",
      "\n",
      "============================================================\n",
      "Processing: 2 - Background and Foundations\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section requires comprehensive citations for technical foundations, empirical evidence, and literature review related to LLM architectures and conversational agents in higher education.\n",
      "Search queries: ['large language model architectures scholarly articles', 'conversational agents in higher education case studies', 'GPT-3 and LLaMA-3 performance research papers']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: large language model architectures scholarly articles\n",
      "Search needed: True\n",
      "Reasoning: The section requires comprehensive citations for technical foundations, empirical evidence, and literature review related to LLM architectures and conversational agents in higher education.\n",
      "Search queries: ['large language model architectures scholarly articles', 'conversational agents in higher education case studies', 'GPT-3 and LLaMA-3 performance research papers']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: large language model architectures scholarly articles\n",
      "  Searching: conversational agents in higher education case studies\n",
      "  Searching: conversational agents in higher education case studies\n",
      "  Searching: GPT-3 and LLaMA-3 performance research papers\n",
      "  Searching: GPT-3 and LLaMA-3 performance research papers\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 2.1 - Domain Background: Conversational Agents in Higher Education\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 2.1 - Domain Background: Conversational Agents in Higher Education\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section is a literature review and background analysis, requiring comprehensive citations to support claims about the evolution, definitions, and impact of conversational agents in higher education.\n",
      "Search queries: ['conversational agents in higher education literature review', 'impact of chatbots on university administration efficiency', 'historical evolution of conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents in higher education literature review\n",
      "Search needed: True\n",
      "Reasoning: The section is a literature review and background analysis, requiring comprehensive citations to support claims about the evolution, definitions, and impact of conversational agents in higher education.\n",
      "Search queries: ['conversational agents in higher education literature review', 'impact of chatbots on university administration efficiency', 'historical evolution of conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents in higher education literature review\n",
      "  Searching: impact of chatbots on university administration efficiency\n",
      "  Searching: impact of chatbots on university administration efficiency\n",
      "  Searching: historical evolution of conversational agents scholarly articles\n",
      "  Searching: historical evolution of conversational agents scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 2.1.1 - Evolution and Definitions of Conversational Agents\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 2.1.1 - Evolution and Definitions of Conversational Agents\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses the evolution of conversational agents, referencing specific technologies and historical milestones, which require academic citations and empirical evidence to support the claims made.\n",
      "Search queries: ['ELIZA chatbot historical impact research papers', 'deep learning conversational agents scholarly articles', 'machine learning conversational agents empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: ELIZA chatbot historical impact research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses the evolution of conversational agents, referencing specific technologies and historical milestones, which require academic citations and empirical evidence to support the claims made.\n",
      "Search queries: ['ELIZA chatbot historical impact research papers', 'deep learning conversational agents scholarly articles', 'machine learning conversational agents empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: ELIZA chatbot historical impact research papers\n",
      "  Searching: deep learning conversational agents scholarly articles\n",
      "  Searching: deep learning conversational agents scholarly articles\n",
      "  Searching: machine learning conversational agents empirical studies\n",
      "  Searching: machine learning conversational agents empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 2.1.2 - Use Cases and Impact in University Administration\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 2.1.2 - Use Cases and Impact in University Administration\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the performance and impact of conversational agents in university administration, which require supporting empirical evidence and studies.\n",
      "Search queries: ['conversational agents university administration performance studies', 'impact of conversational agents on student satisfaction scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents university administration performance studies\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the performance and impact of conversational agents in university administration, which require supporting empirical evidence and studies.\n",
      "Search queries: ['conversational agents university administration performance studies', 'impact of conversational agents on student satisfaction scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents university administration performance studies\n",
      "  Searching: impact of conversational agents on student satisfaction scholarly articles\n",
      "  Searching: impact of conversational agents on student satisfaction scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2 - Technical Foundations: LLM Architectures and Methods\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2 - Technical Foundations: LLM Architectures and Methods\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical methods and claims about performance, requiring authoritative sources and empirical evidence to support the assertions made.\n",
      "Search queries: ['Transformer architecture scholarly articles', 'Pretraining paradigms in LLMs research papers', 'LoRA parameter-efficient fine-tuning studies', 'Supervised Fine-Tuning vs. Direct Preference Optimization empirical evidence', 'Topic modeling and content filtering techniques academic references']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Transformer architecture scholarly articles\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical methods and claims about performance, requiring authoritative sources and empirical evidence to support the assertions made.\n",
      "Search queries: ['Transformer architecture scholarly articles', 'Pretraining paradigms in LLMs research papers', 'LoRA parameter-efficient fine-tuning studies', 'Supervised Fine-Tuning vs. Direct Preference Optimization empirical evidence', 'Topic modeling and content filtering techniques academic references']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Transformer architecture scholarly articles\n",
      "  Searching: Pretraining paradigms in LLMs research papers\n",
      "  Searching: Pretraining paradigms in LLMs research papers\n",
      "  Searching: LoRA parameter-efficient fine-tuning studies\n",
      "  Searching: LoRA parameter-efficient fine-tuning studies\n",
      "  Searching: Supervised Fine-Tuning vs. Direct Preference Optimization empirical evidence\n",
      "  Searching: Supervised Fine-Tuning vs. Direct Preference Optimization empirical evidence\n",
      "  Searching: Topic modeling and content filtering techniques academic references\n",
      "  Searching: Topic modeling and content filtering techniques academic references\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.1 - Transformer Architecture Overview\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.1 - Transformer Architecture Overview\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific technical claims about transformer architecture, empirical evidence regarding performance, and mentions models like GPT-3 and BERT, which require academic references and supporting studies.\n",
      "Search queries: ['transformer architecture empirical studies', 'self-attention mechanism scholarly articles', 'GPT-3 performance metrics research papers', 'BERT language modeling academic references']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: transformer architecture empirical studies\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific technical claims about transformer architecture, empirical evidence regarding performance, and mentions models like GPT-3 and BERT, which require academic references and supporting studies.\n",
      "Search queries: ['transformer architecture empirical studies', 'self-attention mechanism scholarly articles', 'GPT-3 performance metrics research papers', 'BERT language modeling academic references']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: transformer architecture empirical studies\n",
      "  Searching: self-attention mechanism scholarly articles\n",
      "  Searching: self-attention mechanism scholarly articles\n",
      "  Searching: GPT-3 performance metrics research papers\n",
      "  Searching: GPT-3 performance metrics research papers\n",
      "  Searching: BERT language modeling academic references\n",
      "  Searching: BERT language modeling academic references\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.2 - Pretraining Paradigms and Domain Adaptation\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.2 - Pretraining Paradigms and Domain Adaptation\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical paradigms and claims about domain adaptation effectiveness, requiring academic citations to support these claims.\n",
      "Search queries: ['pretraining paradigms in NLP research papers', 'domain adaptation techniques in higher education scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: pretraining paradigms in NLP research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical paradigms and claims about domain adaptation effectiveness, requiring academic citations to support these claims.\n",
      "Search queries: ['pretraining paradigms in NLP research papers', 'domain adaptation techniques in higher education scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: pretraining paradigms in NLP research papers\n",
      "  Searching: domain adaptation techniques in higher education scholarly articles\n",
      "  Searching: domain adaptation techniques in higher education scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 4\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.3 - Parameter-Efficient Fine-Tuning (LoRA)\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 4\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.3 - Parameter-Efficient Fine-Tuning (LoRA)\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical methods and claims about LoRA's effectiveness, requiring academic references and empirical evidence to support the analysis and comparisons.\n",
      "Search queries: ['Low-Rank Adaptation (LoRA) methodology scholarly articles', 'parameter-efficient fine-tuning techniques comparison research papers']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Low-Rank Adaptation (LoRA) methodology scholarly articles\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical methods and claims about LoRA's effectiveness, requiring academic references and empirical evidence to support the analysis and comparisons.\n",
      "Search queries: ['Low-Rank Adaptation (LoRA) methodology scholarly articles', 'parameter-efficient fine-tuning techniques comparison research papers']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Low-Rank Adaptation (LoRA) methodology scholarly articles\n",
      "  Searching: parameter-efficient fine-tuning techniques comparison research papers\n",
      "  Searching: parameter-efficient fine-tuning techniques comparison research papers\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.3.1 - LoRA Methodology and Mechanisms\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.3.1 - LoRA Methodology and Mechanisms\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical methods and claims about LoRA's effectiveness, requiring academic references and empirical evidence to support these assertions.\n",
      "Search queries: ['Low-Rank Adaptation (LoRA) methodology scholarly articles', 'LoRA singular value decomposition empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Low-Rank Adaptation (LoRA) methodology scholarly articles\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical methods and claims about LoRA's effectiveness, requiring academic references and empirical evidence to support these assertions.\n",
      "Search queries: ['Low-Rank Adaptation (LoRA) methodology scholarly articles', 'LoRA singular value decomposition empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Low-Rank Adaptation (LoRA) methodology scholarly articles\n",
      "  Searching: LoRA singular value decomposition empirical studies\n",
      "  Searching: LoRA singular value decomposition empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.3.2 - Comparison with Other PEFT Techniques\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.3.2 - Comparison with Other PEFT Techniques\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about LoRA's efficiency and compares it with other PEFT techniques, requiring empirical evidence and technical citations to support these claims.\n",
      "Search queries: ['LoRA efficiency in NLP tasks research papers', 'PEFT techniques comparison empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: LoRA efficiency in NLP tasks research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about LoRA's efficiency and compares it with other PEFT techniques, requiring empirical evidence and technical citations to support these claims.\n",
      "Search queries: ['LoRA efficiency in NLP tasks research papers', 'PEFT techniques comparison empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: LoRA efficiency in NLP tasks research papers\n",
      "  Searching: PEFT techniques comparison empirical studies\n",
      "  Searching: PEFT techniques comparison empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.4 - Supervised Fine-Tuning vs. Direct Preference Optimization\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.4 - Supervised Fine-Tuning vs. Direct Preference Optimization\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about the effectiveness and methodologies of SFT and DPO, requiring technical citations and empirical evidence to support these assertions.\n",
      "Search queries: ['Supervised Fine-Tuning techniques research papers', 'Direct Preference Optimization empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Supervised Fine-Tuning techniques research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about the effectiveness and methodologies of SFT and DPO, requiring technical citations and empirical evidence to support these assertions.\n",
      "Search queries: ['Supervised Fine-Tuning techniques research papers', 'Direct Preference Optimization empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Supervised Fine-Tuning techniques research papers\n",
      "  Searching: Direct Preference Optimization empirical studies\n",
      "  Searching: Direct Preference Optimization empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.4.1 - Supervised Fine-Tuning (SFT) Techniques\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.4.1 - Supervised Fine-Tuning (SFT) Techniques\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific methodologies and principles of supervised fine-tuning, which require academic references to support claims about its effectiveness and technical foundations.\n",
      "Search queries: ['Supervised Fine-Tuning techniques in NLP research papers', 'Transfer learning in machine learning scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Supervised Fine-Tuning techniques in NLP research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific methodologies and principles of supervised fine-tuning, which require academic references to support claims about its effectiveness and technical foundations.\n",
      "Search queries: ['Supervised Fine-Tuning techniques in NLP research papers', 'Transfer learning in machine learning scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Supervised Fine-Tuning techniques in NLP research papers\n",
      "  Searching: Transfer learning in machine learning scholarly articles\n",
      "  Searching: Transfer learning in machine learning scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 6\n",
      "  Improvements: 6\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.4.2 - Direct Preference Optimization (DPO) Principles\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 6\n",
      "  Improvements: 6\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.4.2 - Direct Preference Optimization (DPO) Principles\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical principles and claims about DPO effectiveness, requiring academic references and empirical evidence to support these assertions.\n",
      "Search queries: ['Direct Preference Optimization principles scholarly articles', 'preference alignment feedback integration empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Direct Preference Optimization principles scholarly articles\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical principles and claims about DPO effectiveness, requiring academic references and empirical evidence to support these assertions.\n",
      "Search queries: ['Direct Preference Optimization principles scholarly articles', 'preference alignment feedback integration empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Direct Preference Optimization principles scholarly articles\n",
      "  Searching: preference alignment feedback integration empirical studies\n",
      "  Searching: preference alignment feedback integration empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.4.3 - Hybrid SFT + DPO Approaches\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.4.3 - Hybrid SFT + DPO Approaches\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the effectiveness and performance improvements of hybrid SFT + DPO approaches, which require supporting empirical evidence and technical citations.\n",
      "Search queries: ['Hybrid SFT + DPO approaches in computational models research papers', 'Simulation Framework Techniques and Dynamic Process Optimization empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Hybrid SFT + DPO approaches in computational models research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the effectiveness and performance improvements of hybrid SFT + DPO approaches, which require supporting empirical evidence and technical citations.\n",
      "Search queries: ['Hybrid SFT + DPO approaches in computational models research papers', 'Simulation Framework Techniques and Dynamic Process Optimization empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Hybrid SFT + DPO approaches in computational models research papers\n",
      "  Searching: Simulation Framework Techniques and Dynamic Process Optimization empirical studies\n",
      "  Searching: Simulation Framework Techniques and Dynamic Process Optimization empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.5 - Topic Modeling and Content Filtering Techniques\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.5 - Topic Modeling and Content Filtering Techniques\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section mentions specific algorithms and models, such as LDA, NMF, and LLMs, which require academic references to support the technical methods and claims about their effectiveness.\n",
      "Search queries: ['Latent Dirichlet Allocation (LDA) research papers', 'Non-negative Matrix Factorization (NMF) scholarly articles', 'Large Language Models theme classification studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Latent Dirichlet Allocation (LDA) research papers\n",
      "Search needed: True\n",
      "Reasoning: The section mentions specific algorithms and models, such as LDA, NMF, and LLMs, which require academic references to support the technical methods and claims about their effectiveness.\n",
      "Search queries: ['Latent Dirichlet Allocation (LDA) research papers', 'Non-negative Matrix Factorization (NMF) scholarly articles', 'Large Language Models theme classification studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Latent Dirichlet Allocation (LDA) research papers\n",
      "  Searching: Non-negative Matrix Factorization (NMF) scholarly articles\n",
      "  Searching: Non-negative Matrix Factorization (NMF) scholarly articles\n",
      "  Searching: Large Language Models theme classification studies\n",
      "  Searching: Large Language Models theme classification studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.5.1 - Unsupervised Topic Modeling\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.5.1 - Unsupervised Topic Modeling\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific algorithms (LDA and NMF) and their applications, which require academic references to support the technical claims and methodologies described.\n",
      "Search queries: ['Latent Dirichlet Allocation research papers', 'Non-negative Matrix Factorization scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Latent Dirichlet Allocation research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific algorithms (LDA and NMF) and their applications, which require academic references to support the technical claims and methodologies described.\n",
      "Search queries: ['Latent Dirichlet Allocation research papers', 'Non-negative Matrix Factorization scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Latent Dirichlet Allocation research papers\n",
      "  Searching: Non-negative Matrix Factorization scholarly articles\n",
      "  Searching: Non-negative Matrix Factorization scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.5.2 - LLM-Based Theme Classification\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 2.2.5.2 - LLM-Based Theme Classification\n",
      "Level: 3\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about the effectiveness and methodologies of LLM-based theme classification, requiring technical citations and empirical evidence to support these assertions.\n",
      "Search queries: ['LLM-based theme classification methods research papers', 'empirical evidence of LLM theme identification accuracy scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: LLM-based theme classification methods research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about the effectiveness and methodologies of LLM-based theme classification, requiring technical citations and empirical evidence to support these assertions.\n",
      "Search queries: ['LLM-based theme classification methods research papers', 'empirical evidence of LLM theme identification accuracy scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: LLM-based theme classification methods research papers\n",
      "  Searching: empirical evidence of LLM theme identification accuracy scholarly articles\n",
      "  Searching: empirical evidence of LLM theme identification accuracy scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 3 - Related Work\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 3 - Related Work\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: This section is a literature review that discusses specific technical methods, empirical evidence, and identifies research gaps, all of which require comprehensive citations from academic sources.\n",
      "Search queries: ['domain-specific large language models in university administration research papers', 'prompt engineering methodologies for conversational agents scholarly articles', 'evaluation metrics for conversational agents in higher education studies', 'research gaps in university management chatbots literature review']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: domain-specific large language models in university administration research papers\n",
      "Search needed: True\n",
      "Reasoning: This section is a literature review that discusses specific technical methods, empirical evidence, and identifies research gaps, all of which require comprehensive citations from academic sources.\n",
      "Search queries: ['domain-specific large language models in university administration research papers', 'prompt engineering methodologies for conversational agents scholarly articles', 'evaluation metrics for conversational agents in higher education studies', 'research gaps in university management chatbots literature review']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: domain-specific large language models in university administration research papers\n",
      "  Searching: prompt engineering methodologies for conversational agents scholarly articles\n",
      "  Searching: prompt engineering methodologies for conversational agents scholarly articles\n",
      "  Searching: evaluation metrics for conversational agents in higher education studies\n",
      "  Searching: evaluation metrics for conversational agents in higher education studies\n",
      "  Searching: research gaps in university management chatbots literature review\n",
      "  Searching: research gaps in university management chatbots literature review\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 3.1 - Domain-Specific LLMs for Administrative Support\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 3.1 - Domain-Specific LLMs for Administrative Support\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the effectiveness and efficiency improvements of domain-specific LLMs, requiring empirical evidence and literature foundation to support these assertions.\n",
      "Search queries: ['domain-specific large language models administrative support research papers', 'LLM efficiency metrics in university administration scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: domain-specific large language models administrative support research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the effectiveness and efficiency improvements of domain-specific LLMs, requiring empirical evidence and literature foundation to support these assertions.\n",
      "Search queries: ['domain-specific large language models administrative support research papers', 'LLM efficiency metrics in university administration scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: domain-specific large language models administrative support research papers\n",
      "  Searching: LLM efficiency metrics in university administration scholarly articles\n",
      "  Searching: LLM efficiency metrics in university administration scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 3.2 - Prompt Engineering for User-Aware Generation\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 3.2 - Prompt Engineering for User-Aware Generation\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific methodologies and claims about prompt engineering's effectiveness, requiring academic references and empirical evidence to support these assertions.\n",
      "Search queries: ['prompt engineering techniques in conversational agents scholarly articles', 'personalized response generation in higher education empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: prompt engineering techniques in conversational agents scholarly articles\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific methodologies and claims about prompt engineering's effectiveness, requiring academic references and empirical evidence to support these assertions.\n",
      "Search queries: ['prompt engineering techniques in conversational agents scholarly articles', 'personalized response generation in higher education empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: prompt engineering techniques in conversational agents scholarly articles\n",
      "  Searching: personalized response generation in higher education empirical studies\n",
      "  Searching: personalized response generation in higher education empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 3.3 - Evaluation Metrics for Conversational Agents\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 3.3 - Evaluation Metrics for Conversational Agents\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses evaluation metrics and claims about their effectiveness, requiring empirical evidence and technical citations to support these assertions.\n",
      "Search queries: ['evaluation metrics for conversational agents in higher education research papers', 'user satisfaction surveys in conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: evaluation metrics for conversational agents in higher education research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses evaluation metrics and claims about their effectiveness, requiring empirical evidence and technical citations to support these assertions.\n",
      "Search queries: ['evaluation metrics for conversational agents in higher education research papers', 'user satisfaction surveys in conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: evaluation metrics for conversational agents in higher education research papers\n",
      "  Searching: user satisfaction surveys in conversational agents scholarly articles\n",
      "  Searching: user satisfaction surveys in conversational agents scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 3.4 - Research Gaps in University Management Chatbots\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 3.4 - Research Gaps in University Management Chatbots\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses research gaps related to scalability and impact of chatbots, requiring empirical evidence and literature foundation to support claims.\n",
      "Search queries: ['scalability of chatbot systems in university environments research papers', 'impact of chatbots on student satisfaction and academic performance scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: scalability of chatbot systems in university environments research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses research gaps related to scalability and impact of chatbots, requiring empirical evidence and literature foundation to support claims.\n",
      "Search queries: ['scalability of chatbot systems in university environments research papers', 'impact of chatbots on student satisfaction and academic performance scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: scalability of chatbot systems in university environments research papers\n",
      "  Searching: impact of chatbots on student satisfaction and academic performance scholarly articles\n",
      "  Searching: impact of chatbots on student satisfaction and academic performance scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 4\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 4 - Data Acquisition and Preprocessing\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 4\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 4 - Data Acquisition and Preprocessing\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section mentions specific technical methods and algorithms such as web scraping, text segmentation, and Mistral's few-shot learning, which require academic references to support their implementation and effectiveness.\n",
      "Search queries: ['web scraping tools for data acquisition in higher education research papers', 'natural language processing text segmentation algorithms scholarly articles', 'Mistral few-shot learning in QA pair generation academic references']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: web scraping tools for data acquisition in higher education research papers\n",
      "Search needed: True\n",
      "Reasoning: The section mentions specific technical methods and algorithms such as web scraping, text segmentation, and Mistral's few-shot learning, which require academic references to support their implementation and effectiveness.\n",
      "Search queries: ['web scraping tools for data acquisition in higher education research papers', 'natural language processing text segmentation algorithms scholarly articles', 'Mistral few-shot learning in QA pair generation academic references']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: web scraping tools for data acquisition in higher education research papers\n",
      "  Searching: natural language processing text segmentation algorithms scholarly articles\n",
      "  Searching: natural language processing text segmentation algorithms scholarly articles\n",
      "  Searching: Mistral few-shot learning in QA pair generation academic references\n",
      "  Searching: Mistral few-shot learning in QA pair generation academic references\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 4.1 - FAU Administrative Data Harvesting\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 4.1 - FAU Administrative Data Harvesting\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical methods like web scraping and metadata tagging systems, which require academic references to support their effectiveness and implementation.\n",
      "Search queries: ['web scraping tools for document retrieval research papers', 'metadata tagging systems in data harvesting scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: web scraping tools for document retrieval research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical methods like web scraping and metadata tagging systems, which require academic references to support their effectiveness and implementation.\n",
      "Search queries: ['web scraping tools for document retrieval research papers', 'metadata tagging systems in data harvesting scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: web scraping tools for document retrieval research papers\n",
      "  Searching: metadata tagging systems in data harvesting scholarly articles\n",
      "  Searching: metadata tagging systems in data harvesting scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 4.2 - Text Segmentation into 4 000-Token Chunks\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 4.2 - Text Segmentation into 4 000-Token Chunks\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical methods for text segmentation and NLP techniques, which require academic references to support the claims and methodologies described.\n",
      "Search queries: ['text segmentation algorithm for large documents scholarly articles', 'natural language processing techniques for semantic coherence research papers']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: text segmentation algorithm for large documents scholarly articles\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical methods for text segmentation and NLP techniques, which require academic references to support the claims and methodologies described.\n",
      "Search queries: ['text segmentation algorithm for large documents scholarly articles', 'natural language processing techniques for semantic coherence research papers']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: text segmentation algorithm for large documents scholarly articles\n",
      "  Searching: natural language processing techniques for semantic coherence research papers\n",
      "  Searching: natural language processing techniques for semantic coherence research papers\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 4.3 - Initial QA-Pair Generation with Mistral\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 4.3 - Initial QA-Pair Generation with Mistral\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical methods and claims about Mistral's capabilities, requiring academic references to support the methodologies and effectiveness of the tool.\n",
      "Search queries: ['Mistral few-shot learning capabilities research papers', 'synthetic QA pair generation in administrative domains scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Mistral few-shot learning capabilities research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical methods and claims about Mistral's capabilities, requiring academic references to support the methodologies and effectiveness of the tool.\n",
      "Search queries: ['Mistral few-shot learning capabilities research papers', 'synthetic QA pair generation in administrative domains scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Mistral few-shot learning capabilities research papers\n",
      "  Searching: synthetic QA pair generation in administrative domains scholarly articles\n",
      "  Searching: synthetic QA pair generation in administrative domains scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 4.4 - Two-Stage Filtering Pipeline\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 4.4 - Two-Stage Filtering Pipeline\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section mentions specific technical methods (BERTopic and LLM-based classification) that require academic references to support their use and effectiveness in the filtering pipeline.\n",
      "Search queries: ['BERTopic topic modeling research papers', 'LLM-based classification techniques in data filtering scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: BERTopic topic modeling research papers\n",
      "Search needed: True\n",
      "Reasoning: The section mentions specific technical methods (BERTopic and LLM-based classification) that require academic references to support their use and effectiveness in the filtering pipeline.\n",
      "Search queries: ['BERTopic topic modeling research papers', 'LLM-based classification techniques in data filtering scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: BERTopic topic modeling research papers\n",
      "  Searching: LLM-based classification techniques in data filtering scholarly articles\n",
      "  Searching: LLM-based classification techniques in data filtering scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 1\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 4.4.1 - Unsupervised Topic Modeling (BERTopic)\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 1\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 4.4.1 - Unsupervised Topic Modeling (BERTopic)\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses the application and effectiveness of BERTopic, an unsupervised topic modeling technique, which requires empirical evidence and technical citations to support claims about its performance and thematic clustering capabilities.\n",
      "Search queries: ['BERTopic algorithm performance in academic datasets', 'BERTopic unsupervised topic modeling effectiveness']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: BERTopic algorithm performance in academic datasets\n",
      "Search needed: True\n",
      "Reasoning: The section discusses the application and effectiveness of BERTopic, an unsupervised topic modeling technique, which requires empirical evidence and technical citations to support claims about its performance and thematic clustering capabilities.\n",
      "Search queries: ['BERTopic algorithm performance in academic datasets', 'BERTopic unsupervised topic modeling effectiveness']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: BERTopic algorithm performance in academic datasets\n",
      "  Searching: BERTopic unsupervised topic modeling effectiveness\n",
      "  Searching: BERTopic unsupervised topic modeling effectiveness\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 1\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 4.4.2 - LLM-Based Theme Classification and Lecture Removal\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 1\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 4.4.2 - LLM-Based Theme Classification and Lecture Removal\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the effectiveness and accuracy of LLMs in thematic classification, which requires empirical evidence and technical citations to support these assertions.\n",
      "Search queries: ['LLM-based theme classification in education research papers', 'Large Language Models lecture content filtering scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: LLM-based theme classification in education research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the effectiveness and accuracy of LLMs in thematic classification, which requires empirical evidence and technical citations to support these assertions.\n",
      "Search queries: ['LLM-based theme classification in education research papers', 'Large Language Models lecture content filtering scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: LLM-based theme classification in education research papers\n",
      "  Searching: Large Language Models lecture content filtering scholarly articles\n",
      "  Searching: Large Language Models lecture content filtering scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 3\n",
      "\n",
      "============================================================\n",
      "Processing: 4.5 - Construction of Chosen vs. Rejected Response Sets\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 3\n",
      "\n",
      "============================================================\n",
      "Processing: 4.5 - Construction of Chosen vs. Rejected Response Sets\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses criteria for selecting and rejecting responses, which may benefit from empirical evidence or technical citations to support the framework's efficacy.\n",
      "Search queries: ['response selection criteria in conversational agents', 'automated response systems thematic relevance scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: response selection criteria in conversational agents\n",
      "Search needed: True\n",
      "Reasoning: The section discusses criteria for selecting and rejecting responses, which may benefit from empirical evidence or technical citations to support the framework's efficacy.\n",
      "Search queries: ['response selection criteria in conversational agents', 'automated response systems thematic relevance scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: response selection criteria in conversational agents\n",
      "  Searching: automated response systems thematic relevance scholarly articles\n",
      "  Searching: automated response systems thematic relevance scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 5 - Model Fine-Tuning and Prompt Engineering\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 5 - Model Fine-Tuning and Prompt Engineering\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific technical claims about QLoRA, DPO, and model performance that require academic references and empirical evidence to support the assertions made.\n",
      "Search queries: ['QLoRA parameter-efficient fine-tuning research papers', 'Direct Preference Optimization (DPO) empirical studies', 'LLaMA 3 and Falcon 7B performance benchmarks scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: QLoRA parameter-efficient fine-tuning research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific technical claims about QLoRA, DPO, and model performance that require academic references and empirical evidence to support the assertions made.\n",
      "Search queries: ['QLoRA parameter-efficient fine-tuning research papers', 'Direct Preference Optimization (DPO) empirical studies', 'LLaMA 3 and Falcon 7B performance benchmarks scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: QLoRA parameter-efficient fine-tuning research papers\n",
      "  Searching: Direct Preference Optimization (DPO) empirical studies\n",
      "  Searching: Direct Preference Optimization (DPO) empirical studies\n",
      "  Searching: LLaMA 3 and Falcon 7B performance benchmarks scholarly articles\n",
      "  Searching: LLaMA 3 and Falcon 7B performance benchmarks scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 5\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.1 - Model Selection: LLaMA 3 and Falcon 7B\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 5\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.1 - Model Selection: LLaMA 3 and Falcon 7B\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about the performance and advantages of LLaMA 3 and Falcon 7B models, as well as the efficacy of role-aware fine-tuning compared to RAG, which require empirical evidence and technical citations to support these assertions.\n",
      "Search queries: ['LLaMA 3 performance metrics scholarly articles', 'Falcon 7B model architecture research papers', 'role-aware fine-tuning vs RAG empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: LLaMA 3 performance metrics scholarly articles\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about the performance and advantages of LLaMA 3 and Falcon 7B models, as well as the efficacy of role-aware fine-tuning compared to RAG, which require empirical evidence and technical citations to support these assertions.\n",
      "Search queries: ['LLaMA 3 performance metrics scholarly articles', 'Falcon 7B model architecture research papers', 'role-aware fine-tuning vs RAG empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: LLaMA 3 performance metrics scholarly articles\n",
      "  Searching: Falcon 7B model architecture research papers\n",
      "  Searching: Falcon 7B model architecture research papers\n",
      "  Searching: role-aware fine-tuning vs RAG empirical studies\n",
      "  Searching: role-aware fine-tuning vs RAG empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 1\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.1.1 - RAG vs. Role-Aware Fine-Tuning: A Design Justification\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 1\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.1.1 - RAG vs. Role-Aware Fine-Tuning: A Design Justification\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about the effectiveness of role-aware fine-tuning compared to RAG, requiring empirical evidence and technical citations to support these claims.\n",
      "Search queries: ['Role-Aware Fine-Tuning research papers', 'Retrieval-Augmented Generation empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Role-Aware Fine-Tuning research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about the effectiveness of role-aware fine-tuning compared to RAG, requiring empirical evidence and technical citations to support these claims.\n",
      "Search queries: ['Role-Aware Fine-Tuning research papers', 'Retrieval-Augmented Generation empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Role-Aware Fine-Tuning research papers\n",
      "  Searching: Retrieval-Augmented Generation empirical studies\n",
      "  Searching: Retrieval-Augmented Generation empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.2 - Supervised Fine-Tuning on Management QA Corpus\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.2 - Supervised Fine-Tuning on Management QA Corpus\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the effectiveness of supervised fine-tuning in management contexts, which requires empirical evidence and academic references to support the methodology and results.\n",
      "Search queries: ['supervised fine-tuning on QA corpus in management contexts research papers', 'effectiveness of fine-tuning models for management queries scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: supervised fine-tuning on QA corpus in management contexts research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the effectiveness of supervised fine-tuning in management contexts, which requires empirical evidence and academic references to support the methodology and results.\n",
      "Search queries: ['supervised fine-tuning on QA corpus in management contexts research papers', 'effectiveness of fine-tuning models for management queries scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: supervised fine-tuning on QA corpus in management contexts research papers\n",
      "  Searching: effectiveness of fine-tuning models for management queries scholarly articles\n",
      "  Searching: effectiveness of fine-tuning models for management queries scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 1\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.3 - Direct Preference Optimization (DPO) with QLoRA\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 1\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.3 - Direct Preference Optimization (DPO) with QLoRA\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific methodologies like Direct Preference Optimization and QLoRA, which require academic references to support claims about their effectiveness and implementation.\n",
      "Search queries: ['Direct Preference Optimization QLoRA research papers', 'DPO QLoRA empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Direct Preference Optimization QLoRA research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific methodologies like Direct Preference Optimization and QLoRA, which require academic references to support claims about their effectiveness and implementation.\n",
      "Search queries: ['Direct Preference Optimization QLoRA research papers', 'DPO QLoRA empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: Direct Preference Optimization QLoRA research papers\n",
      "  Searching: DPO QLoRA empirical studies\n",
      "  Searching: DPO QLoRA empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 5.4 - Custom Prompt Templates and Persona Embedding\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Crawled 0 sources\n",
      "Step 4: No search results to integrate\n",
      "\n",
      "============================================================\n",
      "Processing: 5.4 - Custom Prompt Templates and Persona Embedding\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical methods like persona embedding and adaptive modeling, which require academic references to support claims about their effectiveness and implementation.\n",
      "Search queries: ['persona embedding in conversational agents research papers', 'adaptive persona modeling algorithms scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: persona embedding in conversational agents research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical methods like persona embedding and adaptive modeling, which require academic references to support claims about their effectiveness and implementation.\n",
      "Search queries: ['persona embedding in conversational agents research papers', 'adaptive persona modeling algorithms scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: persona embedding in conversational agents research papers\n",
      "  Searching: adaptive persona modeling algorithms scholarly articles\n",
      "  Searching: adaptive persona modeling algorithms scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 1\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 5.4.1 - Static Persona Injection (User Background & Expert Role)\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 1\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 5.4.1 - Static Persona Injection (User Background & Expert Role)\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the effectiveness and methodologies of static persona injection, which require supporting studies and technical references to validate the approach and its impact on personalization.\n",
      "Search queries: ['static persona injection in AI', 'persona-driven responses in education', 'AI personalization techniques in higher education']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: static persona injection in AI\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the effectiveness and methodologies of static persona injection, which require supporting studies and technical references to validate the approach and its impact on personalization.\n",
      "Search queries: ['static persona injection in AI', 'persona-driven responses in education', 'AI personalization techniques in higher education']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: static persona injection in AI\n",
      "  Searching: persona-driven responses in education\n",
      "  Searching: persona-driven responses in education\n",
      "  Searching: AI personalization techniques in higher education\n",
      "  Searching: AI personalization techniques in higher education\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.4.2 - Role-Based Persona Modeling (Context-Adaptive Generation)\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.4.2 - Role-Based Persona Modeling (Context-Adaptive Generation)\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical methods and claims about adaptive persona models that require academic references and empirical evidence to support the effectiveness and performance of these models.\n",
      "Search queries: ['role-based persona modeling algorithms research papers', 'context-adaptive response generation empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: role-based persona modeling algorithms research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical methods and claims about adaptive persona models that require academic references and empirical evidence to support the effectiveness and performance of these models.\n",
      "Search queries: ['role-based persona modeling algorithms research papers', 'context-adaptive response generation empirical studies']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: role-based persona modeling algorithms research papers\n",
      "  Searching: context-adaptive response generation empirical studies\n",
      "  Searching: context-adaptive response generation empirical studies\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.4.3 - Dynamic Context Windows and Memory Traces\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.4.3 - Dynamic Context Windows and Memory Traces\n",
      "Level: 2\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about the effectiveness of dynamic context windows and memory traces in dialogue systems, which require empirical evidence and technical citations to support these assertions.\n",
      "Search queries: ['dynamic context windows in dialogue systems research papers', 'memory traces in conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: dynamic context windows in dialogue systems research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about the effectiveness of dynamic context windows and memory traces in dialogue systems, which require empirical evidence and technical citations to support these assertions.\n",
      "Search queries: ['dynamic context windows in dialogue systems research papers', 'memory traces in conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: dynamic context windows in dialogue systems research papers\n",
      "  Searching: memory traces in conversational agents scholarly articles\n",
      "  Searching: memory traces in conversational agents scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.5 - Implementation Details and Training Infrastructure\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 5.5 - Implementation Details and Training Infrastructure\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical infrastructure and methodologies, such as high-performance computing clusters and frameworks like TensorFlow and PyTorch, which require academic references to support claims about their effectiveness and efficiency.\n",
      "Search queries: ['high-performance computing clusters GPUs research papers', 'TensorFlow PyTorch training efficiency scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: high-performance computing clusters GPUs research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses specific technical infrastructure and methodologies, such as high-performance computing clusters and frameworks like TensorFlow and PyTorch, which require academic references to support claims about their effectiveness and efficiency.\n",
      "Search queries: ['high-performance computing clusters GPUs research papers', 'TensorFlow PyTorch training efficiency scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: high-performance computing clusters GPUs research papers\n",
      "  Searching: TensorFlow PyTorch training efficiency scholarly articles\n",
      "  Searching: TensorFlow PyTorch training efficiency scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 5\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 6 - Experimental Design and Evaluation\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 5\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 6 - Experimental Design and Evaluation\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about evaluation metrics and model comparisons that require empirical evidence and technical citations to support the methodologies and results discussed.\n",
      "Search queries: ['precision recall F1-score evaluation in conversational agents research papers', 'baseline vs fine-tuned model comparisons in higher education conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: precision recall F1-score evaluation in conversational agents research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about evaluation metrics and model comparisons that require empirical evidence and technical citations to support the methodologies and results discussed.\n",
      "Search queries: ['precision recall F1-score evaluation in conversational agents research papers', 'baseline vs fine-tuned model comparisons in higher education conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: precision recall F1-score evaluation in conversational agents research papers\n",
      "  Searching: baseline vs fine-tuned model comparisons in higher education conversational agents scholarly articles\n",
      "  Searching: baseline vs fine-tuned model comparisons in higher education conversational agents scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 6\n",
      "  Improvements: 6\n",
      "\n",
      "============================================================\n",
      "Processing: 6.1 - Evaluation Metrics: Precision, Recall, F1-Score\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 6\n",
      "  Improvements: 6\n",
      "\n",
      "============================================================\n",
      "Processing: 6.1 - Evaluation Metrics: Precision, Recall, F1-Score\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical evaluation metrics and their application, which requires academic references to support the definitions and methodologies used.\n",
      "Search queries: ['precision recall F1-score evaluation metrics research papers', 'conversational agents performance metrics scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: precision recall F1-score evaluation metrics research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses technical evaluation metrics and their application, which requires academic references to support the definitions and methodologies used.\n",
      "Search queries: ['precision recall F1-score evaluation metrics research papers', 'conversational agents performance metrics scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: precision recall F1-score evaluation metrics research papers\n",
      "  Searching: conversational agents performance metrics scholarly articles\n",
      "  Searching: conversational agents performance metrics scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 1\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 6.2 - Baseline vs. Fine-Tuned Model Comparisons\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 1\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 6.2 - Baseline vs. Fine-Tuned Model Comparisons\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the performance improvements of fine-tuned models over baseline models, which require empirical evidence and technical citations to support these assertions.\n",
      "Search queries: ['fine-tuning models in machine learning research papers', 'baseline vs fine-tuned model performance scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: fine-tuning models in machine learning research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about the performance improvements of fine-tuned models over baseline models, which require empirical evidence and technical citations to support these assertions.\n",
      "Search queries: ['fine-tuning models in machine learning research papers', 'baseline vs fine-tuned model performance scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: fine-tuning models in machine learning research papers\n",
      "  Searching: baseline vs fine-tuned model performance scholarly articles\n",
      "  Searching: baseline vs fine-tuned model performance scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 6.3 - User-Role Simulation and Contextual Tests\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 6.3 - User-Role Simulation and Contextual Tests\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses methodologies and case studies related to user-role simulation and contextual testing, which likely require technical citations and empirical evidence to support the claims made.\n",
      "Search queries: ['user-role simulation in conversational agents research papers', 'contextual testing in AI models scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: user-role simulation in conversational agents research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses methodologies and case studies related to user-role simulation and contextual testing, which likely require technical citations and empirical evidence to support the claims made.\n",
      "Search queries: ['user-role simulation in conversational agents research papers', 'contextual testing in AI models scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: user-role simulation in conversational agents research papers\n",
      "  Searching: contextual testing in AI models scholarly articles\n",
      "  Searching: contextual testing in AI models scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 3 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 1\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 6.4 - Ablation Study on Prompt Components\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 1\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 6.4 - Ablation Study on Prompt Components\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes empirical claims about the impact of prompt components on response accuracy, which requires supporting studies and technical citations.\n",
      "Search queries: ['ablation study on prompt components in conversational agents', 'impact of prompt components on response accuracy in higher education']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: ablation study on prompt components in conversational agents\n",
      "Search needed: True\n",
      "Reasoning: The section makes empirical claims about the impact of prompt components on response accuracy, which requires supporting studies and technical citations.\n",
      "Search queries: ['ablation study on prompt components in conversational agents', 'impact of prompt components on response accuracy in higher education']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: ablation study on prompt components in conversational agents\n",
      "  Searching: impact of prompt components on response accuracy in higher education\n",
      "  Searching: impact of prompt components on response accuracy in higher education\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 6.5 - Statistical Significance and Error Analysis\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 6.5 - Statistical Significance and Error Analysis\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses statistical significance testing and error analysis, which require technical citations and empirical evidence to support the methodologies and claims made.\n",
      "Search queries: ['statistical significance testing methods in conversational agents', 'error analysis in higher education conversational agents']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: statistical significance testing methods in conversational agents\n",
      "Search needed: True\n",
      "Reasoning: The section discusses statistical significance testing and error analysis, which require technical citations and empirical evidence to support the methodologies and claims made.\n",
      "Search queries: ['statistical significance testing methods in conversational agents', 'error analysis in higher education conversational agents']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: statistical significance testing methods in conversational agents\n",
      "  Searching: error analysis in higher education conversational agents\n",
      "  Searching: error analysis in higher education conversational agents\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 7 - Results and Discussion\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 4\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 7 - Results and Discussion\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about performance improvements and limitations, which require supporting empirical evidence and technical citations.\n",
      "Search queries: ['precision and F1-score improvement in conversational agents in higher education', 'limitations of conversational agents in administrative systems']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: precision and F1-score improvement in conversational agents in higher education\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about performance improvements and limitations, which require supporting empirical evidence and technical citations.\n",
      "Search queries: ['precision and F1-score improvement in conversational agents in higher education', 'limitations of conversational agents in administrative systems']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: precision and F1-score improvement in conversational agents in higher education\n",
      "  Searching: limitations of conversational agents in administrative systems\n",
      "  Searching: limitations of conversational agents in administrative systems\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 7.1 - Quantitative Performance Gains (+10 % Precision, +12 % F1)\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 7.1 - Quantitative Performance Gains (+10 % Precision, +12 % F1)\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about performance improvements and statistical methods used, which require supporting empirical evidence and technical citations.\n",
      "Search queries: ['precision and F1-score improvements in conversational agents', 'paired t-tests in conversational agent performance analysis']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: precision and F1-score improvements in conversational agents\n",
      "Search needed: True\n",
      "Reasoning: The section makes specific claims about performance improvements and statistical methods used, which require supporting empirical evidence and technical citations.\n",
      "Search queries: ['precision and F1-score improvements in conversational agents', 'paired t-tests in conversational agent performance analysis']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: precision and F1-score improvements in conversational agents\n",
      "  Searching: paired t-tests in conversational agent performance analysis\n",
      "  Searching: paired t-tests in conversational agent performance analysis\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 7.2 - Qualitative Case Studies and Exemplars\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 7.2 - Qualitative Case Studies and Exemplars\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses thematic analysis and qualitative case studies, which require academic references to support the methodology and claims about agent effectiveness.\n",
      "Search queries: ['thematic analysis in conversational agents research papers', 'qualitative case studies in conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: thematic analysis in conversational agents research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses thematic analysis and qualitative case studies, which require academic references to support the methodology and claims about agent effectiveness.\n",
      "Search queries: ['thematic analysis in conversational agents research papers', 'qualitative case studies in conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: thematic analysis in conversational agents research papers\n",
      "  Searching: qualitative case studies in conversational agents scholarly articles\n",
      "  Searching: qualitative case studies in conversational agents scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 7.3 - Limitations of the Current Approach\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 7.3 - Limitations of the Current Approach\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section discusses limitations and performance of conversational agents, which requires empirical evidence and technical citations to support claims made about their effectiveness and shortcomings.\n",
      "Search queries: ['conversational agents limitations in higher education research papers', 'performance evaluation of conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents limitations in higher education research papers\n",
      "Search needed: True\n",
      "Reasoning: The section discusses limitations and performance of conversational agents, which requires empirical evidence and technical citations to support claims made about their effectiveness and shortcomings.\n",
      "Search queries: ['conversational agents limitations in higher education research papers', 'performance evaluation of conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents limitations in higher education research papers\n",
      "  Searching: performance evaluation of conversational agents scholarly articles\n",
      "  Searching: performance evaluation of conversational agents scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 1 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 3\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 7.4 - Implications for University Administrative Workflows\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 3\n",
      "  Improvements: 4\n",
      "\n",
      "============================================================\n",
      "Processing: 7.4 - Implications for University Administrative Workflows\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes empirical claims about the impact of conversational agents on administrative workflows, requiring supporting studies and data from scholarly articles.\n",
      "Search queries: ['conversational agents impact on university administrative workflows scholarly articles', 'performance metrics of conversational agents in higher education research papers']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents impact on university administrative workflows scholarly articles\n",
      "Search needed: True\n",
      "Reasoning: The section makes empirical claims about the impact of conversational agents on administrative workflows, requiring supporting studies and data from scholarly articles.\n",
      "Search queries: ['conversational agents impact on university administrative workflows scholarly articles', 'performance metrics of conversational agents in higher education research papers']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents impact on university administrative workflows scholarly articles\n",
      "  Searching: performance metrics of conversational agents in higher education research papers\n",
      "  Searching: performance metrics of conversational agents in higher education research papers\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 5\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 8 - Conclusion and Future Work\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 5\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 8 - Conclusion and Future Work\n",
      "Level: 0\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: False\n",
      "Reasoning: The conclusion and future work section is primarily a synthesis of the thesis findings and does not make specific technical or empirical claims requiring external citations.\n",
      "No search needed - keeping original draft\n",
      "\n",
      "============================================================\n",
      "Processing: 8.1 - Summary of Contributions\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: False\n",
      "Reasoning: The conclusion and future work section is primarily a synthesis of the thesis findings and does not make specific technical or empirical claims requiring external citations.\n",
      "No search needed - keeping original draft\n",
      "\n",
      "============================================================\n",
      "Processing: 8.1 - Summary of Contributions\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: False\n",
      "Reasoning: The section is a summary of contributions and does not make specific technical or empirical claims that require citations or external validation.\n",
      "No search needed - keeping original draft\n",
      "\n",
      "============================================================\n",
      "Processing: 8.2 - Recommendations for Deployment\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: False\n",
      "Reasoning: The section is a summary of contributions and does not make specific technical or empirical claims that require citations or external validation.\n",
      "No search needed - keeping original draft\n",
      "\n",
      "============================================================\n",
      "Processing: 8.2 - Recommendations for Deployment\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about deployment strategies and their effectiveness, requiring empirical evidence and technical citations to support the recommendations.\n",
      "Search queries: ['conversational agents deployment strategies research papers', 'scalability and user engagement in conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents deployment strategies research papers\n",
      "Search needed: True\n",
      "Reasoning: The section makes claims about deployment strategies and their effectiveness, requiring empirical evidence and technical citations to support the recommendations.\n",
      "Search queries: ['conversational agents deployment strategies research papers', 'scalability and user engagement in conversational agents scholarly articles']\n",
      "Step 2: Performing web searches...\n",
      "  Searching: conversational agents deployment strategies research papers\n",
      "  Searching: scalability and user engagement in conversational agents scholarly articles\n",
      "  Searching: scalability and user engagement in conversational agents scholarly articles\n",
      "Step 3: Processing search results...\n",
      "Step 3: Processing search results...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Crawled 2 sources\n",
      "Step 4: Enhancing content with research...\n",
      "  Citations added: 2\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 8.3 - Directions for Further Research\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "  Citations added: 2\n",
      "  Improvements: 5\n",
      "\n",
      "============================================================\n",
      "Processing: 8.3 - Directions for Further Research\n",
      "Level: 1\n",
      "Step 1: Analyzing search requirements...\n",
      "Search needed: False\n",
      "Reasoning: The section outlines directions for future research without making specific technical or empirical claims that require citations.\n",
      "No search needed - keeping original draft\n",
      "\n",
      "Saving enhanced hierarchy...\n",
      "\n",
      "============================================================\n",
      "ENHANCEMENT SUMMARY\n",
      "============================================================\n",
      "1 - Introduction [✓ ENHANCED] (1 sources)\n",
      "  1.1 - Motivation and Objectives [✓ ENHANCED] (3 sources)\n",
      "  1.2 - Thesis Scope and Contributions [✓ ENHANCED] (2 sources)\n",
      "  1.3 - Thesis Organization [○ ORIGINAL] (0 sources)\n",
      "2 - Background and Foundations [✓ ENHANCED] (0 sources)\n",
      "  2.1 - Domain Background: Conversational Agents in Higher Education [✓ ENHANCED] (0 sources)\n",
      "    2.1.1 - Evolution and Definitions of Conversational Agents [✓ ENHANCED] (0 sources)\n",
      "    2.1.2 - Use Cases and Impact in University Administration [✓ ENHANCED] (2 sources)\n",
      "  2.2 - Technical Foundations: LLM Architectures and Methods [✓ ENHANCED] (0 sources)\n",
      "    2.2.1 - Transformer Architecture Overview [✓ ENHANCED] (1 sources)\n",
      "    2.2.2 - Pretraining Paradigms and Domain Adaptation [✓ ENHANCED] (1 sources)\n",
      "    2.2.3 - Parameter-Efficient Fine-Tuning (LoRA) [✓ ENHANCED] (0 sources)\n",
      "      2.2.3.1 - LoRA Methodology and Mechanisms [✓ ENHANCED] (1 sources)\n",
      "      2.2.3.2 - Comparison with Other PEFT Techniques [✓ ENHANCED] (2 sources)\n",
      "    2.2.4 - Supervised Fine-Tuning vs. Direct Preference Optimization [✓ ENHANCED] (3 sources)\n",
      "      2.2.4.1 - Supervised Fine-Tuning (SFT) Techniques [✓ ENHANCED] (2 sources)\n",
      "      2.2.4.2 - Direct Preference Optimization (DPO) Principles [✓ ENHANCED] (1 sources)\n",
      "      2.2.4.3 - Hybrid SFT + DPO Approaches [✓ ENHANCED] (2 sources)\n",
      "    2.2.5 - Topic Modeling and Content Filtering Techniques [✓ ENHANCED] (2 sources)\n",
      "      2.2.5.1 - Unsupervised Topic Modeling [✓ ENHANCED] (2 sources)\n",
      "      2.2.5.2 - LLM-Based Theme Classification [✓ ENHANCED] (1 sources)\n",
      "3 - Related Work [✓ ENHANCED] (0 sources)\n",
      "  3.1 - Domain-Specific LLMs for Administrative Support [✓ ENHANCED] (2 sources)\n",
      "  3.2 - Prompt Engineering for User-Aware Generation [✓ ENHANCED] (1 sources)\n",
      "  3.3 - Evaluation Metrics for Conversational Agents [✓ ENHANCED] (1 sources)\n",
      "  3.4 - Research Gaps in University Management Chatbots [✓ ENHANCED] (2 sources)\n",
      "4 - Data Acquisition and Preprocessing [✓ ENHANCED] (0 sources)\n",
      "  4.1 - FAU Administrative Data Harvesting [✓ ENHANCED] (3 sources)\n",
      "  4.2 - Text Segmentation into 4 000-Token Chunks [✓ ENHANCED] (2 sources)\n",
      "  4.3 - Initial QA-Pair Generation with Mistral [✓ ENHANCED] (2 sources)\n",
      "  4.4 - Two-Stage Filtering Pipeline [✓ ENHANCED] (1 sources)\n",
      "    4.4.1 - Unsupervised Topic Modeling (BERTopic) [✓ ENHANCED] (3 sources)\n",
      "    4.4.2 - LLM-Based Theme Classification and Lecture Removal [✓ ENHANCED] (2 sources)\n",
      "  4.5 - Construction of Chosen vs. Rejected Response Sets [✓ ENHANCED] (0 sources)\n",
      "5 - Model Fine-Tuning and Prompt Engineering [✓ ENHANCED] (1 sources)\n",
      "  5.1 - Model Selection: LLaMA 3 and Falcon 7B [✓ ENHANCED] (3 sources)\n",
      "    5.1.1 - RAG vs. Role-Aware Fine-Tuning: A Design Justification [✓ ENHANCED] (2 sources)\n",
      "  5.2 - Supervised Fine-Tuning on Management QA Corpus [✓ ENHANCED] (2 sources)\n",
      "  5.3 - Direct Preference Optimization (DPO) with QLoRA [✓ ENHANCED] (0 sources)\n",
      "  5.4 - Custom Prompt Templates and Persona Embedding [✓ ENHANCED] (2 sources)\n",
      "    5.4.1 - Static Persona Injection (User Background & Expert Role) [✓ ENHANCED] (1 sources)\n",
      "    5.4.2 - Role-Based Persona Modeling (Context-Adaptive Generation) [✓ ENHANCED] (2 sources)\n",
      "    5.4.3 - Dynamic Context Windows and Memory Traces [✓ ENHANCED] (1 sources)\n",
      "  5.5 - Implementation Details and Training Infrastructure [✓ ENHANCED] (1 sources)\n",
      "6 - Experimental Design and Evaluation [✓ ENHANCED] (1 sources)\n",
      "  6.1 - Evaluation Metrics: Precision, Recall, F1-Score [✓ ENHANCED] (2 sources)\n",
      "  6.2 - Baseline vs. Fine-Tuned Model Comparisons [✓ ENHANCED] (2 sources)\n",
      "  6.3 - User-Role Simulation and Contextual Tests [✓ ENHANCED] (3 sources)\n",
      "  6.4 - Ablation Study on Prompt Components [✓ ENHANCED] (1 sources)\n",
      "  6.5 - Statistical Significance and Error Analysis [✓ ENHANCED] (2 sources)\n",
      "7 - Results and Discussion [✓ ENHANCED] (2 sources)\n",
      "  7.1 - Quantitative Performance Gains (+10 % Precision, +12 % F1) [✓ ENHANCED] (2 sources)\n",
      "  7.2 - Qualitative Case Studies and Exemplars [✓ ENHANCED] (1 sources)\n",
      "  7.3 - Limitations of the Current Approach [✓ ENHANCED] (1 sources)\n",
      "  7.4 - Implications for University Administrative Workflows [✓ ENHANCED] (2 sources)\n",
      "8 - Conclusion and Future Work [○ ORIGINAL] (0 sources)\n",
      "  8.1 - Summary of Contributions [○ ORIGINAL] (0 sources)\n",
      "  8.2 - Recommendations for Deployment [✓ ENHANCED] (2 sources)\n",
      "  8.3 - Directions for Further Research [○ ORIGINAL] (0 sources)\n",
      "\n",
      "Enhanced hierarchy saved to: section_hierarchy_enhanced.json\n",
      "Search needed: False\n",
      "Reasoning: The section outlines directions for future research without making specific technical or empirical claims that require citations.\n",
      "No search needed - keeping original draft\n",
      "\n",
      "Saving enhanced hierarchy...\n",
      "\n",
      "============================================================\n",
      "ENHANCEMENT SUMMARY\n",
      "============================================================\n",
      "1 - Introduction [✓ ENHANCED] (1 sources)\n",
      "  1.1 - Motivation and Objectives [✓ ENHANCED] (3 sources)\n",
      "  1.2 - Thesis Scope and Contributions [✓ ENHANCED] (2 sources)\n",
      "  1.3 - Thesis Organization [○ ORIGINAL] (0 sources)\n",
      "2 - Background and Foundations [✓ ENHANCED] (0 sources)\n",
      "  2.1 - Domain Background: Conversational Agents in Higher Education [✓ ENHANCED] (0 sources)\n",
      "    2.1.1 - Evolution and Definitions of Conversational Agents [✓ ENHANCED] (0 sources)\n",
      "    2.1.2 - Use Cases and Impact in University Administration [✓ ENHANCED] (2 sources)\n",
      "  2.2 - Technical Foundations: LLM Architectures and Methods [✓ ENHANCED] (0 sources)\n",
      "    2.2.1 - Transformer Architecture Overview [✓ ENHANCED] (1 sources)\n",
      "    2.2.2 - Pretraining Paradigms and Domain Adaptation [✓ ENHANCED] (1 sources)\n",
      "    2.2.3 - Parameter-Efficient Fine-Tuning (LoRA) [✓ ENHANCED] (0 sources)\n",
      "      2.2.3.1 - LoRA Methodology and Mechanisms [✓ ENHANCED] (1 sources)\n",
      "      2.2.3.2 - Comparison with Other PEFT Techniques [✓ ENHANCED] (2 sources)\n",
      "    2.2.4 - Supervised Fine-Tuning vs. Direct Preference Optimization [✓ ENHANCED] (3 sources)\n",
      "      2.2.4.1 - Supervised Fine-Tuning (SFT) Techniques [✓ ENHANCED] (2 sources)\n",
      "      2.2.4.2 - Direct Preference Optimization (DPO) Principles [✓ ENHANCED] (1 sources)\n",
      "      2.2.4.3 - Hybrid SFT + DPO Approaches [✓ ENHANCED] (2 sources)\n",
      "    2.2.5 - Topic Modeling and Content Filtering Techniques [✓ ENHANCED] (2 sources)\n",
      "      2.2.5.1 - Unsupervised Topic Modeling [✓ ENHANCED] (2 sources)\n",
      "      2.2.5.2 - LLM-Based Theme Classification [✓ ENHANCED] (1 sources)\n",
      "3 - Related Work [✓ ENHANCED] (0 sources)\n",
      "  3.1 - Domain-Specific LLMs for Administrative Support [✓ ENHANCED] (2 sources)\n",
      "  3.2 - Prompt Engineering for User-Aware Generation [✓ ENHANCED] (1 sources)\n",
      "  3.3 - Evaluation Metrics for Conversational Agents [✓ ENHANCED] (1 sources)\n",
      "  3.4 - Research Gaps in University Management Chatbots [✓ ENHANCED] (2 sources)\n",
      "4 - Data Acquisition and Preprocessing [✓ ENHANCED] (0 sources)\n",
      "  4.1 - FAU Administrative Data Harvesting [✓ ENHANCED] (3 sources)\n",
      "  4.2 - Text Segmentation into 4 000-Token Chunks [✓ ENHANCED] (2 sources)\n",
      "  4.3 - Initial QA-Pair Generation with Mistral [✓ ENHANCED] (2 sources)\n",
      "  4.4 - Two-Stage Filtering Pipeline [✓ ENHANCED] (1 sources)\n",
      "    4.4.1 - Unsupervised Topic Modeling (BERTopic) [✓ ENHANCED] (3 sources)\n",
      "    4.4.2 - LLM-Based Theme Classification and Lecture Removal [✓ ENHANCED] (2 sources)\n",
      "  4.5 - Construction of Chosen vs. Rejected Response Sets [✓ ENHANCED] (0 sources)\n",
      "5 - Model Fine-Tuning and Prompt Engineering [✓ ENHANCED] (1 sources)\n",
      "  5.1 - Model Selection: LLaMA 3 and Falcon 7B [✓ ENHANCED] (3 sources)\n",
      "    5.1.1 - RAG vs. Role-Aware Fine-Tuning: A Design Justification [✓ ENHANCED] (2 sources)\n",
      "  5.2 - Supervised Fine-Tuning on Management QA Corpus [✓ ENHANCED] (2 sources)\n",
      "  5.3 - Direct Preference Optimization (DPO) with QLoRA [✓ ENHANCED] (0 sources)\n",
      "  5.4 - Custom Prompt Templates and Persona Embedding [✓ ENHANCED] (2 sources)\n",
      "    5.4.1 - Static Persona Injection (User Background & Expert Role) [✓ ENHANCED] (1 sources)\n",
      "    5.4.2 - Role-Based Persona Modeling (Context-Adaptive Generation) [✓ ENHANCED] (2 sources)\n",
      "    5.4.3 - Dynamic Context Windows and Memory Traces [✓ ENHANCED] (1 sources)\n",
      "  5.5 - Implementation Details and Training Infrastructure [✓ ENHANCED] (1 sources)\n",
      "6 - Experimental Design and Evaluation [✓ ENHANCED] (1 sources)\n",
      "  6.1 - Evaluation Metrics: Precision, Recall, F1-Score [✓ ENHANCED] (2 sources)\n",
      "  6.2 - Baseline vs. Fine-Tuned Model Comparisons [✓ ENHANCED] (2 sources)\n",
      "  6.3 - User-Role Simulation and Contextual Tests [✓ ENHANCED] (3 sources)\n",
      "  6.4 - Ablation Study on Prompt Components [✓ ENHANCED] (1 sources)\n",
      "  6.5 - Statistical Significance and Error Analysis [✓ ENHANCED] (2 sources)\n",
      "7 - Results and Discussion [✓ ENHANCED] (2 sources)\n",
      "  7.1 - Quantitative Performance Gains (+10 % Precision, +12 % F1) [✓ ENHANCED] (2 sources)\n",
      "  7.2 - Qualitative Case Studies and Exemplars [✓ ENHANCED] (1 sources)\n",
      "  7.3 - Limitations of the Current Approach [✓ ENHANCED] (1 sources)\n",
      "  7.4 - Implications for University Administrative Workflows [✓ ENHANCED] (2 sources)\n",
      "8 - Conclusion and Future Work [○ ORIGINAL] (0 sources)\n",
      "  8.1 - Summary of Contributions [○ ORIGINAL] (0 sources)\n",
      "  8.2 - Recommendations for Deployment [✓ ENHANCED] (2 sources)\n",
      "  8.3 - Directions for Further Research [○ ORIGINAL] (0 sources)\n",
      "\n",
      "Enhanced hierarchy saved to: section_hierarchy_enhanced.json\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Enhanced Section Pipeline with Web Search & Content Integration\n",
    "# ===============================\n",
    "import json\n",
    "import re\n",
    "from typing import List, Optional, Dict, Any\n",
    "from pydantic import BaseModel\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import trafilatura\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# ===============================\n",
    "# 1. MODELS\n",
    "# ===============================\n",
    "class SectionNode(BaseModel):\n",
    "    section: str\n",
    "    title: str\n",
    "    level: int\n",
    "    children: List[\"SectionNode\"] = []\n",
    "    draft_content: Optional[str] = None\n",
    "    needs_search: Optional[bool] = None\n",
    "    search_queries: Optional[List[str]] = None\n",
    "    search_results: Optional[List[Dict[str, Any]]] = None\n",
    "    enhanced_content: Optional[str] = None  # Final enhanced content with citations\n",
    "\n",
    "SectionNode.update_forward_refs()\n",
    "\n",
    "class SearchDecision(BaseModel):\n",
    "    needs_search: bool\n",
    "    queries: List[str]\n",
    "    reasoning: str\n",
    "\n",
    "class ContentEnhancement(BaseModel):\n",
    "    enhanced_content: str\n",
    "    citations_added: List[str]\n",
    "    improvements_made: List[str]\n",
    "\n",
    "# ===============================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ===============================\n",
    "def load_hierarchy(path: str) -> List[SectionNode]:\n",
    "    \"\"\"Load section hierarchy from JSON file\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return [SectionNode(**s) for s in data]\n",
    "\n",
    "def save_hierarchy(path: str, hierarchy: List[SectionNode]):\n",
    "    \"\"\"Save section hierarchy to JSON file\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([s.model_dump() for s in hierarchy], f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ===============================\n",
    "# 3. SEARCH DECISION LOGIC\n",
    "# ===============================\n",
    "def decide_search_for_section(\n",
    "    section: SectionNode,\n",
    "    llm_client,\n",
    "    model_name: str\n",
    ") -> SearchDecision:\n",
    "    \"\"\"Analyze section draft and decide if web search is needed\"\"\"\n",
    "    \n",
    "    draft = section.draft_content or \"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an academic research assistant for a Masters thesis in Data Science about \"Conversational Agents in Higher Education\".\n",
    "\n",
    "SECTION: {section.section} - {section.title} (Level {section.level})\n",
    "\n",
    "DRAFT CONTENT:\n",
    "{draft}\n",
    "\n",
    "STRICT SEARCH CRITERIA - Only search if the section SPECIFICALLY needs:\n",
    "\n",
    "1. TECHNICAL CITATIONS: Does this section mention specific technical methods, algorithms, or frameworks that need academic references?\n",
    "2. EMPIRICAL EVIDENCE: Does this section make claims about performance, effectiveness, or results that need supporting studies?\n",
    "3. LITERATURE FOUNDATION: Is this a literature review, related work, or background section that requires comprehensive citations?\n",
    "4. STATISTICAL DATA: Does this section mention statistics or metrics that need source attribution?\n",
    "\n",
    "DO NOT SEARCH FOR:\n",
    "- Organizational/structural sections (thesis organization, conclusion summaries)\n",
    "- General introductory statements without specific claims\n",
    "- Methodology descriptions that are self-contained\n",
    "- Sections that are primarily descriptive of your own work\n",
    "\n",
    "SEARCH DECISION:\n",
    "- Introduction sections: ONLY if they make specific technical or empirical claims\n",
    "- Background/Literature sections: YES - these need comprehensive citations\n",
    "- Technical foundation sections: YES - need authoritative sources for methods\n",
    "- Methodology sections: ONLY if referencing established methods/frameworks\n",
    "- Results/Discussion: ONLY if comparing to other studies or citing benchmarks\n",
    "- Conclusion/Organization: NO - these don't need external citations\n",
    "\n",
    "If search is needed, create 1-2 HIGHLY SPECIFIC search queries using exact technical terms from the draft.\n",
    "Format queries as: \"exact technical term\" + \"research papers\" or \"scholarly articles\"\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "- needs_search: true/false\n",
    "- queries: [\"specific technical query\"] (empty if no search)\n",
    "- reasoning: One sentence explaining your decision\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm_client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=SearchDecision,\n",
    "            temperature=0.2,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        return response.choices[0].message.parsed\n",
    "    except Exception as e:\n",
    "        print(f\"Error in search decision for section {section.section}: {e}\")\n",
    "        return SearchDecision(needs_search=False, queries=[], reasoning=\"Error in analysis\")\n",
    "\n",
    "# ===============================\n",
    "# 4. WEB SEARCH & SCRAPING\n",
    "# ===============================\n",
    "def bing_search(\n",
    "    query: str,\n",
    "    subscription_key: str,\n",
    "    endpoint: str,\n",
    "    count: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Perform Bing web search\"\"\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "    \n",
    "    # Enhanced academic search query\n",
    "    academic_query = f'\"{query}\" filetype:pdf OR site:arxiv.org OR site:ieee.org OR site:acm.org OR site:springer.com OR site:sciencedirect.com OR site:researchgate.net'\n",
    "    \n",
    "    params = {\n",
    "        \"q\": academic_query,\n",
    "        \"count\": count,\n",
    "        \"responseFilter\": \"webPages\",\n",
    "        \"safeSearch\": \"Strict\"\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        response = requests.get(endpoint, headers=headers, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        for i, item in enumerate(data.get(\"webPages\", {}).get(\"value\", [])):\n",
    "            url = item.get(\"url\", \"\")\n",
    "            title = item.get(\"name\", \"\")\n",
    "            snippet = item.get(\"snippet\", \"\")\n",
    "            \n",
    "            # Prioritize academic sources\n",
    "            is_academic = any(domain in url.lower() for domain in [\n",
    "                'arxiv.org', 'ieee.org', 'acm.org', 'springer.com', \n",
    "                'elsevier.com', 'sciencedirect.com', 'researchgate.net',\n",
    "                'scholar.google.com', 'doi.org', 'pubmed.ncbi.nlm.nih.gov',\n",
    "                'jstor.org', 'tandfonline.com', 'wiley.com'\n",
    "            ])\n",
    "            \n",
    "            # Also check for PDF files (often research papers)\n",
    "            is_pdf = url.lower().endswith('.pdf') or 'filetype:pdf' in title.lower()\n",
    "            \n",
    "            # Check for academic keywords in title/snippet\n",
    "            academic_keywords = ['research', 'study', 'analysis', 'evaluation', 'framework', 'model', 'algorithm', 'methodology']\n",
    "            has_academic_keywords = any(keyword in (title + snippet).lower() for keyword in academic_keywords)\n",
    "            \n",
    "            if is_academic or is_pdf or has_academic_keywords:\n",
    "                results.append({\n",
    "                    \"url\": url,\n",
    "                    \"title\": title,\n",
    "                    \"snippet\": snippet,\n",
    "                    \"rank\": i,\n",
    "                    \"is_academic\": is_academic,\n",
    "                    \"is_pdf\": is_pdf,\n",
    "                    \"academic_score\": sum([is_academic, is_pdf, has_academic_keywords])\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Bing search failed for query '{query}': {e}\")\n",
    "    \n",
    "    # Sort by academic score (highest first), then by rank\n",
    "    results.sort(key=lambda x: (-x.get(\"academic_score\", 0), x.get(\"rank\", 999)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def is_allowed_by_robots(url: str, user_agent: str = \"*\") -> bool:\n",
    "    \"\"\"Check if URL is allowed by robots.txt\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        robots_url = urljoin(f\"{parsed.scheme}://{parsed.netloc}\", \"/robots.txt\")\n",
    "        rp = RobotFileParser()\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp.can_fetch(user_agent, url)\n",
    "    except:\n",
    "        return True  # If can't check, assume it's allowed\n",
    "\n",
    "def crawl_url(url: str) -> str:\n",
    "    \"\"\"Extract content from URL using trafilatura\"\"\"\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if downloaded:\n",
    "            content = trafilatura.extract(downloaded, include_comments=False, include_tables=True)\n",
    "            return content[:5000] if content else \"\"  # Limit content length\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to crawl {url}: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "def process_search_results(search_results: List[Dict[str, Any]], max_crawl: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process and crawl search results\"\"\"\n",
    "    # Sort by academic sources first, then by rank\n",
    "    sorted_results = sorted(search_results, key=lambda x: (not x.get(\"is_academic\", False), x.get(\"rank\", 999)))\n",
    "    \n",
    "    # Check robots.txt and crawl allowed URLs\n",
    "    crawlable_results = []\n",
    "    for result in sorted_results[:max_crawl * 2]:  # Check more than we need\n",
    "        if is_allowed_by_robots(result[\"url\"]):\n",
    "            result[\"can_crawl\"] = True\n",
    "            crawlable_results.append(result)\n",
    "        if len(crawlable_results) >= max_crawl:\n",
    "            break\n",
    "    \n",
    "    # Crawl URLs in parallel\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        future_to_result = {\n",
    "            executor.submit(crawl_url, result[\"url\"]): result \n",
    "            for result in crawlable_results\n",
    "        }\n",
    "        \n",
    "        crawled_results = []\n",
    "        for future in as_completed(future_to_result):\n",
    "            result = future_to_result[future]\n",
    "            try:\n",
    "                content = future.result()\n",
    "                if content and len(content.strip()) > 100:  # Only keep substantial content\n",
    "                    result[\"crawled_content\"] = content\n",
    "                    crawled_results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error crawling {result['url']}: {e}\")\n",
    "    \n",
    "    return crawled_results\n",
    "\n",
    "# ===============================\n",
    "# 5. CONTENT ENHANCEMENT\n",
    "# ===============================\n",
    "def enhance_section_with_research(\n",
    "    section: SectionNode,\n",
    "    llm_client,\n",
    "    model_name: str\n",
    ") -> ContentEnhancement:\n",
    "    \"\"\"Enhance section content with research findings and proper citations\"\"\"\n",
    "    \n",
    "    draft = section.draft_content or \"\"\n",
    "    search_results = section.search_results or []\n",
    "    \n",
    "    # Prepare research context\n",
    "    research_context = []\n",
    "    for i, result in enumerate(search_results, 1):\n",
    "        research_context.append(f\"\"\"\n",
    "SOURCE {i}:\n",
    "Title: {result.get('title', 'Unknown')}\n",
    "URL: {result.get('url', '')}\n",
    "Snippet: {result.get('snippet', '')}\n",
    "Content: {result.get('crawled_content', '')[:1500]}...\n",
    "Academic Score: {result.get('academic_score', 0)}\n",
    "\"\"\")\n",
    "    \n",
    "    research_text = \"\\n\".join(research_context)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert academic writer enhancing a Masters thesis section with research citations.\n",
    "\n",
    "THESIS CONTEXT: \"Conversational Agents in Higher Education\" - Data Science Masters at Friedrich Alexander University\n",
    "\n",
    "SECTION: {section.section} - {section.title} (Level {section.level})\n",
    "\n",
    "ORIGINAL DRAFT:\n",
    "{draft}\n",
    "\n",
    "RESEARCH SOURCES:\n",
    "{research_text}\n",
    "\n",
    "ENHANCEMENT GUIDELINES:\n",
    "1. SELECTIVE INTEGRATION: Only integrate research that directly supports or extends the draft content\n",
    "2. ACADEMIC CITATIONS: Use proper format (Author, Year) or [Reference Number] \n",
    "3. STRENGTHEN CLAIMS: Back factual statements with evidence from credible sources\n",
    "4. MAINTAIN FLOW: Keep the original structure and writing style\n",
    "5. ADD VALUE: Include specific statistics, methodologies, or findings that enhance understanding\n",
    "6. INDICATE IMAGES: Where technical concepts need visualization, add \"[Image needed: description]\"\n",
    "\n",
    "QUALITY STANDARDS:\n",
    "- Prioritize sources with higher academic scores\n",
    "- Only cite sources that are genuinely relevant to the section content\n",
    "- Don't force citations where they don't naturally fit\n",
    "- Maintain academic rigor while keeping readability\n",
    "- Add specific technical details from the research where appropriate\n",
    "\n",
    "SPECIAL INSTRUCTIONS:\n",
    "- If discussing technical methods, include implementation details from sources\n",
    "- For background sections, focus on recent developments and key studies\n",
    "- For methodology sections, cite authoritative sources for techniques used\n",
    "- Add \"[Image needed: X]\" where diagrams, architectures, or charts would help explain concepts\n",
    "\n",
    "OUTPUT REQUIREMENTS:\n",
    "- enhanced_content: Improved section with integrated research and proper citations\n",
    "- citations_added: List of specific citations you added with brief relevance explanation\n",
    "- improvements_made: List of key enhancements made to strengthen the academic quality\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm_client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=ContentEnhancement,\n",
    "            temperature=0.3,\n",
    "            max_tokens=4096\n",
    "        )\n",
    "        return response.choices[0].message.parsed\n",
    "    except Exception as e:\n",
    "        print(f\"Error enhancing section {section.section}: {e}\")\n",
    "        return ContentEnhancement(\n",
    "            enhanced_content=draft,\n",
    "            citations_added=[],\n",
    "            improvements_made=[\"Error occurred during enhancement\"]\n",
    "        )\n",
    "\n",
    "# ===============================\n",
    "# 6. MAIN PROCESSING PIPELINE\n",
    "# ===============================\n",
    "def process_section_with_research(\n",
    "    section: SectionNode,\n",
    "    llm_client,\n",
    "    model_name: str,\n",
    "    bing_key: str,\n",
    "    bing_endpoint: str\n",
    ") -> None:\n",
    "    \"\"\"Process a single section: decide search -> search -> enhance\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {section.section} - {section.title}\")\n",
    "    print(f\"Level: {section.level}\")\n",
    "    \n",
    "    # Step 1: Decide if search is needed\n",
    "    print(\"Step 1: Analyzing search requirements...\")\n",
    "    decision = decide_search_for_section(section, llm_client, model_name)\n",
    "    section.needs_search = decision.needs_search\n",
    "    section.search_queries = decision.queries\n",
    "    \n",
    "    print(f\"Search needed: {decision.needs_search}\")\n",
    "    print(f\"Reasoning: {decision.reasoning}\")\n",
    "    \n",
    "    if decision.needs_search and decision.queries:\n",
    "        print(f\"Search queries: {decision.queries}\")\n",
    "        \n",
    "        # Step 2: Perform web search\n",
    "        print(\"Step 2: Performing web searches...\")\n",
    "        all_results = []\n",
    "        for query in decision.queries:\n",
    "            print(f\"  Searching: {query}\")\n",
    "            results = bing_search(query, bing_key, bing_endpoint)\n",
    "            all_results.extend(results)\n",
    "            time.sleep(1)  # Rate limiting\n",
    "        \n",
    "        # Step 3: Process and crawl results\n",
    "        print(\"Step 3: Processing search results...\")\n",
    "        section.search_results = process_search_results(all_results)\n",
    "        print(f\"  Crawled {len(section.search_results)} sources\")\n",
    "        \n",
    "        # Step 4: Enhance content\n",
    "        if section.search_results:\n",
    "            print(\"Step 4: Enhancing content with research...\")\n",
    "            enhancement = enhance_section_with_research(section, llm_client, model_name)\n",
    "            section.enhanced_content = enhancement.enhanced_content\n",
    "            print(f\"  Citations added: {len(enhancement.citations_added)}\")\n",
    "            print(f\"  Improvements: {len(enhancement.improvements_made)}\")\n",
    "        else:\n",
    "            print(\"Step 4: No search results to integrate\")\n",
    "            section.enhanced_content = section.draft_content\n",
    "    else:\n",
    "        print(\"No search needed - keeping original draft\")\n",
    "        section.enhanced_content = section.draft_content\n",
    "        section.search_results = []\n",
    "\n",
    "def process_hierarchy_with_research(\n",
    "    nodes: List[SectionNode],\n",
    "    llm_client,\n",
    "    model_name: str,\n",
    "    bing_key: str,\n",
    "    bing_endpoint: str\n",
    ") -> None:\n",
    "    \"\"\"Recursively process all sections in the hierarchy\"\"\"\n",
    "    \n",
    "    for node in nodes:\n",
    "        if node.draft_content:  # Only process sections with drafts\n",
    "            process_section_with_research(node, llm_client, model_name, bing_key, bing_endpoint)\n",
    "        \n",
    "        # Process children\n",
    "        if node.children:\n",
    "            process_hierarchy_with_research(node.children, llm_client, model_name, bing_key, bing_endpoint)\n",
    "\n",
    "# ===============================\n",
    "# 7. MAIN EXECUTION\n",
    "# ===============================\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    BING_KEY = \"a7079abbee3b4a12a6db317f03bf13ca\"\n",
    "    BING_ENDPOINT = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "    \n",
    "    print(\"Loading section hierarchy with drafts...\")\n",
    "    hierarchy = load_hierarchy(\"section_hierarchy_with_drafts.json\")\n",
    "    \n",
    "    print(f\"Loaded {len(hierarchy)} root sections\")\n",
    "    \n",
    "    # Process all sections\n",
    "    print(\"\\nStarting research enhancement pipeline...\")\n",
    "    process_hierarchy_with_research(\n",
    "        hierarchy,\n",
    "        llm_client,\n",
    "        model_name,\n",
    "        BING_KEY,\n",
    "        BING_ENDPOINT\n",
    "    )\n",
    "    \n",
    "    # Save enhanced hierarchy\n",
    "    print(\"\\nSaving enhanced hierarchy...\")\n",
    "    save_hierarchy(\"section_hierarchy_enhanced.json\", hierarchy)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENHANCEMENT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def print_summary(nodes, indent=0):\n",
    "        for node in nodes:\n",
    "            if node.draft_content:\n",
    "                prefix = \"  \" * indent\n",
    "                search_status = \"✓ ENHANCED\" if node.needs_search else \"○ ORIGINAL\"\n",
    "                search_count = len(node.search_results) if node.search_results else 0\n",
    "                print(f\"{prefix}{node.section} - {node.title} [{search_status}] ({search_count} sources)\")\n",
    "            \n",
    "            if node.children:\n",
    "                print_summary(node.children, indent + 1)\n",
    "    \n",
    "    print_summary(hierarchy)\n",
    "    print(f\"\\nEnhanced hierarchy saved to: section_hierarchy_enhanced.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a82799e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting thesis markdown generation...\n",
      "\n",
      "📝 Generating simple thesis markdown...\n",
      "✅ Complete thesis saved as: Complete_Thesis.md\n",
      "📄 Total length: 197120 characters\n",
      "📚 Sections processed: 59\n",
      "\n",
      "📊 Generating detailed thesis markdown with statistics...\n",
      "✅ Detailed thesis saved as: Complete_Thesis_Detailed.md\n",
      "📊 Statistics:\n",
      "   📄 Total sections: 63\n",
      "   ✨ Enhanced sections: 59 (93.7%)\n",
      "   📝 Total words: 25,919\n",
      "   📚 Citations: 65\n",
      "   🖼️ Image placeholders: 51\n",
      "\n",
      "✅ Thesis markdown generation complete!\n",
      "📁 Files created:\n",
      "   • Complete_Thesis.md (simple version)\n",
      "   • Complete_Thesis_Detailed.md (with statistics)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# MARKDOWN THESIS GENERATOR (FROM ENHANCED JSON)\n",
    "# ===============================\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "def generate_markdown_from_enhanced_json(\n",
    "    input_file: str = \"section_hierarchy_enhanced.json\",\n",
    "    output_file: str = \"Complete_Thesis.md\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert enhanced section hierarchy JSON to complete thesis markdown\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the enhanced hierarchy\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    sections = [SectionNode(**s) for s in data]\n",
    "    \n",
    "    # Start building markdown content\n",
    "    markdown_content = []\n",
    "    \n",
    "    # Add thesis header\n",
    "    markdown_content.append(\"\"\"# Conversational Agents in Higher Education\n",
    "## A Data Science Approach to University Administrative Support\n",
    "\n",
    "**Masters Thesis in Data Science**  \n",
    "Friedrich Alexander University, Erlangen, Germany\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\")\n",
    "    \n",
    "    def process_node(node: SectionNode, depth: int = 0):\n",
    "        \"\"\"Recursively process each section node\"\"\"\n",
    "        \n",
    "        # Determine markdown heading level (# ## ### etc.)\n",
    "        heading_level = \"#\" * (depth + 1)\n",
    "        \n",
    "        # Add section heading\n",
    "        markdown_content.append(f\"{heading_level} {node.section} {node.title}\\n\")\n",
    "        \n",
    "        # Add the enhanced content (or draft if no enhancement)\n",
    "        content = node.enhanced_content or node.draft_content or \"\"\n",
    "        if content.strip():\n",
    "            markdown_content.append(f\"{content}\\n\")\n",
    "        \n",
    "        # Add some spacing\n",
    "        markdown_content.append(\"\\n\")\n",
    "        \n",
    "        # Process children recursively\n",
    "        for child in node.children:\n",
    "            process_node(child, depth + 1)\n",
    "    \n",
    "    # Process all root sections\n",
    "    for section in sections:\n",
    "        process_node(section)\n",
    "    \n",
    "    # Add bibliography placeholder\n",
    "    markdown_content.append(\"\"\"---\n",
    "\n",
    "## References\n",
    "\n",
    "[References will be populated based on the citations added during enhancement]\n",
    "\n",
    "---\n",
    "\n",
    "## List of Figures\n",
    "\n",
    "[Figure references will be populated based on \"[Image needed: X]\" placeholders in the content]\n",
    "\n",
    "---\n",
    "\n",
    "## List of Tables\n",
    "\n",
    "[Table references will be populated based on content analysis]\n",
    "\n",
    "---\n",
    "\n",
    "## Appendices\n",
    "\n",
    "[Additional materials and supplementary content]\n",
    "\n",
    "\"\"\")\n",
    "    \n",
    "    # Join all content and save\n",
    "    final_markdown = \"\\n\".join(markdown_content)\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(final_markdown)\n",
    "    \n",
    "    print(f\"✅ Complete thesis saved as: {output_file}\")\n",
    "    print(f\"📄 Total length: {len(final_markdown)} characters\")\n",
    "    \n",
    "    # Print summary of sections processed\n",
    "    def count_sections(nodes, level=0):\n",
    "        count = 0\n",
    "        for node in nodes:\n",
    "            if node.enhanced_content or node.draft_content:\n",
    "                count += 1\n",
    "            count += count_sections(node.children, level + 1)\n",
    "        return count\n",
    "    \n",
    "    total_sections = count_sections(sections)\n",
    "    print(f\"📚 Sections processed: {total_sections}\")\n",
    "    \n",
    "    return final_markdown\n",
    "\n",
    "# ===============================\n",
    "# ENHANCED MARKDOWN GENERATOR (WITH METADATA)\n",
    "# ===============================\n",
    "def generate_detailed_markdown_with_stats(\n",
    "    input_file: str = \"section_hierarchy_enhanced.json\",\n",
    "    output_file: str = \"Complete_Thesis_Detailed.md\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate markdown with additional metadata and statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the enhanced hierarchy\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    sections = [SectionNode(**s) for s in data]\n",
    "    \n",
    "    # Collect statistics\n",
    "    stats = {\n",
    "        \"total_sections\": 0,\n",
    "        \"enhanced_sections\": 0,\n",
    "        \"citations_count\": 0,\n",
    "        \"image_placeholders\": 0,\n",
    "        \"total_words\": 0\n",
    "    }\n",
    "    \n",
    "    markdown_content = []\n",
    "    \n",
    "    # Add comprehensive header\n",
    "    markdown_content.append(f\"\"\"# Conversational Agents in Higher Education\n",
    "## A Data Science Approach to University Administrative Support\n",
    "\n",
    "**Masters Thesis in Data Science**  \n",
    "Friedrich Alexander University, Erlangen, Germany\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Supervisor:** [Supervisor Name]  \n",
    "**Date:** {__import__('datetime').datetime.now().strftime('%B %Y')}\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This thesis explores the development and implementation of conversational agents specifically designed for higher education administrative support. Through advanced data science methodologies and large language model fine-tuning, we present a comprehensive framework for enhancing university administrative efficiency while maintaining academic rigor and user satisfaction.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\"\"\")\n",
    "    \n",
    "    # Generate table of contents\n",
    "    def generate_toc(nodes, depth=0):\n",
    "        toc_lines = []\n",
    "        for node in nodes:\n",
    "            indent = \"  \" * depth\n",
    "            toc_lines.append(f\"{indent}- [{node.section} {node.title}](#{node.section.lower().replace('.', '')}-{node.title.lower().replace(' ', '-').replace(':', '').replace('(', '').replace(')', '')})\")\n",
    "            if node.children:\n",
    "                toc_lines.extend(generate_toc(node.children, depth + 1))\n",
    "        return toc_lines\n",
    "    \n",
    "    toc = generate_toc(sections)\n",
    "    markdown_content.extend(toc)\n",
    "    markdown_content.append(\"\\n---\\n\")\n",
    "    \n",
    "    def process_node_detailed(node: SectionNode, depth: int = 0):\n",
    "        \"\"\"Process node with detailed statistics tracking\"\"\"\n",
    "        \n",
    "        # Update statistics\n",
    "        stats[\"total_sections\"] += 1\n",
    "        \n",
    "        # Determine markdown heading level\n",
    "        heading_level = \"#\" * (depth + 1)\n",
    "        \n",
    "        # Create anchor-friendly section ID\n",
    "        section_id = f\"{node.section.lower().replace('.', '')}-{node.title.lower().replace(' ', '-').replace(':', '').replace('(', '').replace(')', '')}\"\n",
    "        \n",
    "        # Add section heading with anchor\n",
    "        markdown_content.append(f'{heading_level} {node.section} {node.title} {{#{section_id}}}\\n')\n",
    "        \n",
    "        # Get content\n",
    "        content = node.enhanced_content or node.draft_content or \"\"\n",
    "        \n",
    "        if content.strip():\n",
    "            # Track statistics\n",
    "            if node.enhanced_content:\n",
    "                stats[\"enhanced_sections\"] += 1\n",
    "            \n",
    "            # Count citations (look for patterns like (Author, Year) or [Reference])\n",
    "            citation_patterns = len(__import__('re').findall(r'\\([A-Za-z]+,?\\s+\\d{4}\\)|\\[\\d+\\]|\\[Reference\\s+\\d+\\]', content))\n",
    "            stats[\"citations_count\"] += citation_patterns\n",
    "            \n",
    "            # Count image placeholders\n",
    "            image_patterns = len(__import__('re').findall(r'\\[Image needed:.*?\\]', content))\n",
    "            stats[\"image_placeholders\"] += image_patterns\n",
    "            \n",
    "            # Count words\n",
    "            word_count = len(content.split())\n",
    "            stats[\"total_words\"] += word_count\n",
    "            \n",
    "            # Add content with metadata comment\n",
    "            markdown_content.append(f\"<!-- Section {node.section}: {word_count} words, {citation_patterns} citations, {image_patterns} images -->\\n\")\n",
    "            markdown_content.append(f\"{content}\\n\")\n",
    "        \n",
    "        # Add spacing\n",
    "        markdown_content.append(\"\\n\")\n",
    "        \n",
    "        # Process children\n",
    "        for child in node.children:\n",
    "            process_node_detailed(child, depth + 1)\n",
    "    \n",
    "    # Process all sections\n",
    "    for section in sections:\n",
    "        process_node_detailed(section)\n",
    "    \n",
    "    # Add comprehensive footer\n",
    "    markdown_content.append(f\"\"\"---\n",
    "\n",
    "## Document Statistics\n",
    "\n",
    "- **Total Sections:** {stats['total_sections']}\n",
    "- **Enhanced Sections:** {stats['enhanced_sections']} ({stats['enhanced_sections']/stats['total_sections']*100:.1f}%)\n",
    "- **Total Word Count:** {stats['total_words']:,} words\n",
    "- **Citations Added:** {stats['citations_count']}\n",
    "- **Image Placeholders:** {stats['image_placeholders']}\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "*Note: This section should be populated with the actual references based on citations used throughout the thesis.*\n",
    "\n",
    "---\n",
    "\n",
    "## List of Figures\n",
    "\n",
    "*Note: Figures should be created based on the {stats['image_placeholders']} \"[Image needed: X]\" placeholders identified in the content.*\n",
    "\n",
    "---\n",
    "\n",
    "## Appendices\n",
    "\n",
    "### Appendix A: Technical Implementation Details\n",
    "### Appendix B: Data Processing Workflows  \n",
    "### Appendix C: Evaluation Metrics and Results\n",
    "### Appendix D: Source Code and Documentation\n",
    "\n",
    "---\n",
    "\n",
    "*Generated on {__import__('datetime').datetime.now().strftime('%B %d, %Y at %H:%M')}*\n",
    "\"\"\")\n",
    "    \n",
    "    # Save the file\n",
    "    final_markdown = \"\\n\".join(markdown_content)\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(final_markdown)\n",
    "    \n",
    "    print(f\"✅ Detailed thesis saved as: {output_file}\")\n",
    "    print(f\"📊 Statistics:\")\n",
    "    print(f\"   📄 Total sections: {stats['total_sections']}\")\n",
    "    print(f\"   ✨ Enhanced sections: {stats['enhanced_sections']} ({stats['enhanced_sections']/stats['total_sections']*100:.1f}%)\")\n",
    "    print(f\"   📝 Total words: {stats['total_words']:,}\")\n",
    "    print(f\"   📚 Citations: {stats['citations_count']}\")\n",
    "    print(f\"   🖼️ Image placeholders: {stats['image_placeholders']}\")\n",
    "    \n",
    "    return final_markdown\n",
    "\n",
    "# ===============================\n",
    "# EXECUTION FUNCTIONS\n",
    "# ===============================\n",
    "def create_thesis_markdown():\n",
    "    \"\"\"Create both simple and detailed markdown versions\"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting thesis markdown generation...\")\n",
    "    \n",
    "    # Check if enhanced file exists\n",
    "    import os\n",
    "    if not os.path.exists(\"section_hierarchy_enhanced.json\"):\n",
    "        print(\"❌ Error: section_hierarchy_enhanced.json not found!\")\n",
    "        print(\"   Please run the enhancement pipeline first.\")\n",
    "        return\n",
    "    \n",
    "    # Generate simple version\n",
    "    print(\"\\n📝 Generating simple thesis markdown...\")\n",
    "    generate_markdown_from_enhanced_json()\n",
    "    \n",
    "    # Generate detailed version\n",
    "    print(\"\\n📊 Generating detailed thesis markdown with statistics...\")\n",
    "    generate_detailed_markdown_with_stats()\n",
    "    \n",
    "    print(\"\\n✅ Thesis markdown generation complete!\")\n",
    "    print(\"📁 Files created:\")\n",
    "    print(\"   • Complete_Thesis.md (simple version)\")\n",
    "    print(\"   • Complete_Thesis_Detailed.md (with statistics)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_thesis_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3802f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
